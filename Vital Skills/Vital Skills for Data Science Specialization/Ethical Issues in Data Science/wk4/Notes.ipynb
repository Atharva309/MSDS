{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a430768",
   "metadata": {},
   "source": [
    "# Why algorithms can be racist and sexist\n",
    "\n",
    "https://www.vox.com/recode/2020/2/18/21121286/algorithms-bias-discrimination-facial-recognition-transparency\"\n",
    "\n",
    "Humans are error-prone and biased, but that doesn’t mean that algorithms are necessarily better. Still, the tech is already making important decisions about your life and potentially ruling over which political advertisements you see, how your application to your dream job is screened, how police officers are deployed in your neighborhood, and even predicting your home’s risk of fire.\n",
    "\n",
    "But these systems can be biased based on who builds them, how they’re developed, and how they’re ultimately used. This is commonly known as algorithmic bias. It’s tough to figure out exactly how systems might be susceptible to algorithmic bias, especially since this technology often operates in a corporate black box. We frequently don’t know how a particular \n",
    "\n",
    "Nicol Turner-Lee, a Center for Technology Innovation fellow at the Brookings Institution think tank, explains that we can think about algorithmic bias in two primary ways: accuracy and impact. An AI can have different accuracy rates for different demographic groups. Similarly, an algorithm can make vastly different decisions when applied to different populations.\n",
    "\n",
    "Importantly, when you think of data, you might think of formal studies in which demographics and representation are carefully considered, limitations are weighed, and then the results are peer-reviewed. That’s not necessarily the case with the AI-based systems that might be used to make a decision about you. Let’s take one source of data everyone has access to: the internet. One study found that, by teaching an artificial intelligence to crawl through the internet — and just reading what humans have already written — the system would produce prejudices against black people and women. \n",
    "\n",
    "Another example of how training data can produce sexism in an algorithm occurred a few years ago, when Amazon tried to use AI to build a résumé-screening tool. According to Reuters, the company’s hope was that technology could make the process of sorting through job applications more efficient. It built a screening algorithm using résumés the company had collected for a decade, but those résumés tended to come from men. That meant the system, in the end, learned to discriminate against women. It also ended up factoring in proxies for gender, like whether an applicant went to a women’s college. (Amazon says the tool was never used and that it was nonfunctional for several reasons.)\n",
    "\n",
    "Here’s another thing to keep in mind: Just because a tool is tested for bias — which assumes that engineers who are checking for bias actually understand how bias manifests and operates — against one group doesn’t mean it is tested for bias against another type of group. This is also true when an algorithm is considering several types of identity factors at the same time: A tool may deemed fairly accurate on white women, for instance, but that doesn’t necessarily mean it works with black women. \n",
    "\n",
    "In some cases, it might be impossible to find training data free of bias. Take historical data produced by the United States criminal justice system. It’s hard to imagine that data produced by an institution rife with systemic racism could be used to build out an effective and fair tool. As researchers at New York University and the AI Now Institute outline, predictive policing tools can be fed “dirty data,” including policing patterns that reflect police departments’ conscious and implicit biases, as well as police corruption.\n",
    "\n",
    "Transparency is a first step for accountability\n",
    "\n",
    "One of the reasons algorithmic bias can seem so opaque is because, on our own, we usually can’t tell when it’s happening (or if an algorithm is even in the mix). That was one of the reasons why the controversy over a husband and wife who both applied for an Apple Card — and got widely different credit limits — attracted so much attention, Turner-Lee says. It was a rare instance in which two people, who at least appeared to be exposed to the same algorithm and could easily compare notes. The details of this case still aren’t clear, though the company’s credit card is now being investigated by regulators. \n",
    "\n",
    "Companies will claim to be accurate, overall, but won’t always reveal their training data (remember, that’s the data that the artificial intelligence trains on before evaluating new data, like, say, your job application). Many don’t appear to be subjecting themselves to audit by a third-party evaluator or publicly sharing how their systems fare when applied to different demographic groups. Some researchers, such as Joy Buolamwini and Timnit Gebru, say that sharing this demographic information about both the data used to train and the data used to check artificial intelligence should be a baseline definition of transparency.\n",
    "\n",
    "The Equal Employment Opportunity Commission, which investigates employment discrimination, is reportedly looking into at least two cases involving algorithmic discrimination. At the same time, the White House is encouraging federal agencies that are figuring out how to regulate artificial intelligence to keep technological innovation in mind. That raises the challenge of whether the government is prepared to study and govern this technology, and figure out how existing laws apply. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbcd842",
   "metadata": {},
   "source": [
    "# Why did the A-level algorithm say no?\n",
    "\n",
    "https://www.bbc.com/news/education-53787203\n",
    "\n",
    "Accusations of unfairness over this year's A-level results in England have focused on an \"algorithm\" for deciding results of exams cancelled by the pandemic.\n",
    "\n",
    "There have been two key pieces of information used to produce estimated grades: how students have been ranked in ability and how well their school or college has performed in exams in recent years.\n",
    "\n",
    "So the results were produced by combining the ranking of pupils with the share of grades expected in their school. There were other minor adjustments, but those were the shaping factors.\n",
    "\n",
    "The independent schools and the high-achieving state schools with strongest track records of exams were always going to collect the winners' medals, because it was an action replay of the last few years' races.\n",
    "\n",
    "And those in struggling schools were going to see their potential grades capped once again by the underachievement of previous years.\n",
    "\n",
    "\n",
    "These predictions were collected in England too - but were discounted as being the deciding factor, because they were so generous that it would have meant a huge increase in top grades, up to 38%.\n",
    "\n",
    "And the \"downgrading\" of almost 40% of results has reflected the lowering of teachers' predictions back to the levels that previous history suggests would have been achieved.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d914a787",
   "metadata": {},
   "source": [
    "# The Algorithms Aren’t Biased, We Are \n",
    "\n",
    "https://medium.com/mit-media-lab/the-algorithms-arent-biased-we-are-a691f5f6f6f2\n",
    "\n",
    "This matters in a lot of ways. “Algorithmic bias” is showing up all over the press right now. What does that term mean? Algorithms are doling out discriminatory sentence recommendations for judges to use. Algorithms are baking in gender stereotypes to translation services. Algorithms are pushing viewers towards extremist videos on YouTube. Most folks I know agree this is not the world we want.\n",
    "\n",
    "They reflect the biases in our questions and our data. These biases get baked into machine learning projects in both feature selection and training data. This is on us, not the computers.\n",
    "\n",
    "I’m heartened by examples like Microsoft’s efforts to undo gender bias in publicly available language models (trying to solve the “doctors are men” problem). I love my colleague Joy Buolamwini’s efforts to reframe this as a question of “justice” in the social and technical intervention she calls the “Algorithmic Justice League” (video). ProPublica’s investigative reporting is holding companies accountable for their discriminatory sentencing predictions. The amazing Zeynep Tufekci is leading the way in speaking and writing about the danger this poses to society at large. Cathy O’Neil’s Weapons of Math Destruction documents the myriad of implications for this, raising a warning flag for society at large. Fields like law are debating the implications of algorithm-driven decision making in public policy settings. City ordinances are starting to tackle the question of how to legislate against some of the effects I’ve described.\n",
    "\n",
    "These efforts can hopefully serve as “corrective lenses” for these algorithmic mirrors — addressing the troubling aspects we see in our own reflections. The key here is to remember that it is up to us to do something about this. Determining a decision with an algorithm doesn’t automatically make it reliable and trustworthy; just like quantifying something with data doesn’t automatically make it true. We need to look at our own reflections in these algorithmic mirrors and make sure we see the future we want to see.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740166d9",
   "metadata": {},
   "source": [
    "# Want Less-Biased Decisions? Use Algorithms.\n",
    "\n",
    "https://hbr.org/2018/07/want-less-biased-decisions-use-algorithms?utm_source=twitter&utm_medium=social&utm_campaign=hbr&source=post_page---------------------------\n",
    "\n",
    "What Does the Research Say?\n",
    "\n",
    "There is a large body of research on algorithmic decision making that dates back several decades. And the existing studies on this topic all have a remarkably similar conclusion: Algorithms are less biased and more accurate than the humans they are replacing. \n",
    "\n",
    "\n",
    "    In 2002 a team of economists studied the impact of automated underwriting algorithms in the mortgage lending industry. Their primary findings were “that [automated underwriting] systems more accurately predict default than manual underwriters do” and “that this increased accuracy results in higher borrower approval rates, especially for underserved applicants.” Rather than marginalizing traditionally underserved home buyers, the algorithmic system actually benefited this segment of consumers the most.\n",
    "    A similar conclusion was reached by Bo Cowgill at Columbia Business School when he studied the performance of a job-screening algorithm at a software company (forthcoming research). When the company rolled out the algorithm to decide which applicants should get interviews, the algorithm actually favored “nontraditional” candidates much more than human screeners did. Compared with the humans, the algorithm exhibited significantly less bias against candidates that were underrepresented at the firm (such as those without personal referrals or degrees from prestigious universities).\n",
    "    In the context of New York City pre-trial bail hearings, a team of prominent computer scientists and economists determined that algorithms have the potential to achieve significantly more-equitable decisions than the judges who currently make bail decisions, with “jailing rate reductions [of] up to 41.9% with no increase in crime rates.” They also found that in their model “all categories of crime, including violent crimes, show reductions [in jailing rates]; and these gains can be achieved while simultaneously reducing racial disparities.”\n",
    "    The New York Times Magazine recently reported a longform story to answer the question, “Can an algorithm tell when kids are in danger?” It turns out the answer is “yes,” and that algorithms can perform this task much more accurately than humans. Rather than exacerbating the pernicious racial biases associated with some government services, “the Allegheny experience suggests that its screening tool is less bad at weighing biases than human screeners have been.”\n",
    "    Lastly, by looking at historical data on publicly traded companies, a team of finance professors set out to build an algorithm to choose the best board members for a given company. Not only did the researchers find that companies would perform better with algorithmically selected board members, but compared with their proposed algorithm, they “found that firms [without algorithms] tend to choose directors who are much more likely to be male, have a large network, have a lot of board experience, currently serve on more boards, and have a finance background.”\n",
    "\n",
    "\n",
    "In each of these case studies, the data scientists did what sounds like an alarming thing: They trained their algorithms on past data that is surely biased by historical prejudices.\n",
    "\n",
    "In all the examples mentioned above, the humans who used to make decisions were so remarkably bad that replacing them with algorithms both increased accuracy and reduced institutional biases. This is what economists call a Pareto improvement, where one policy beats out the alternative on every outcome we care about. While many critics like to imply that modern organizations pursue the operational efficiency and greater productivity at the expense of equity and fairness, all available evidence in these contexts suggests that there is no such trade-off: Algorithms deliver more-efficient and more-equitable outcomes. If anything should alarm you, it should be the fact that so many important decisions are being made by human beings who we know are inconsistent, biased, and phenomenally bad decision makers.\n",
    "\n",
    "There is even an academic conference every year at which researchers not only discuss the ethical and social challenges of machine learning but also present new models and methods for ensuring algorithms have a positive impact on society. This work will likely become even more important as less-transparent algorithms like deep learning become more common.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124e1776",
   "metadata": {},
   "source": [
    "# New York City couldn’t pry open its own black box algorithms. So now what?\n",
    "\n",
    "https://www.vox.com/recode/2019/12/18/21026229/nyc-ai-algorithms-shadow-report\n",
    "\n",
    "\n",
    "Algorithms make decisions that could impact nearly 9 million New Yorkers, but we don’t actually know much about them. So in 2017, the city council decided that the public should have some insight into these black-box calculations — which can influence everything from policing to where your kid goes to school — and passed a law creating a task force to investigate the city’s so-called “automated decision systems” (ADSs) and propose ideas for their regulation.\n",
    "\n",
    "Algorithms and artificial intelligence can influence much of a city government’s operations. Predictive models and algorithms have been used to do everything from improving public school bus routes and predicting a home’s risk of fire to determining the likelihood of whether a child has been exposed to lead paint. In New York City, it’s publicly known that such systems have been used to predict which landlords are likely to harass their tenants, in the evaluation of teacher performance, and in the DNA analysis used by the criminal justice system, examples that were flagged by the research nonprofit AI Now.\n",
    "\n",
    "In its concluding report released last month, the task force recommended that the city create a centralized “structure” for agencies using automated decision systems that could also help determine best management practices. The task force further called for more public education on algorithm-based systems and for the development of protocols for publicly revealing some information about ADSs, among other broad proposals.\n",
    "\n",
    "Richardson emphasizes that transparency isn’t just a value in itself, but is also key for investigating the social, ethical, and fairness concerns that government use of algorithms can raise. Critics are especially concerned that algorithms can cause a disparate impact on members of protected groups, who — without information about how many of these tools actually work — have few avenues of recourse against an unfair or biased decision. \n",
    "\n",
    "Earlier this month, Richardson and AI Now released a shadow report critiquing and analyzing the task force’s work, and highlighted its own recommendations for moving forward. Those include recommended policy safeguards and procedures, as well as tailored proposals for city agencies, including the New York Police Department, the Administration of Children’s Services, and the Department of Education on their individual use of algorithm-based systems. \n",
    "\n",
    "In New York’s state Senate, there’s also proposed legislation that would create a task force looking at “automated decision systems” for the state agencies. \n",
    "\n",
    "State governments are leading the charge for greater AI transparency\n",
    "\n",
    "Across the country, there are other efforts at promoting transparency in government use of algorithms. Vermont is in the midst of a study that, in part, focuses on the AI used by its state government. Alabama has established a commission on artificial intelligence that’s due to be released in May of next year. \n",
    "\n",
    "He says the definition of automated decision-making systems used in the bill will likely be changed and possibly narrowed in order to make the legislation easier for agencies to understand and comply with. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe56039",
   "metadata": {},
   "source": [
    "# Amazon scraps secret AI recruiting tool that showed bias against women\n",
    "\n",
    "https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G\n",
    "\n",
    "\n",
    "That is because Amazon’s computer models were trained to vet applicants by observing patterns in resumes submitted to the company over a 10-year period. Most came from men, a reflection of male dominance across the tech industry.\n",
    "\n",
    "Some 55 percent of U.S. human resources managers said artificial intelligence, or AI, would be a regular part of their work within the next five years, according to a 2017 survey by talent software firm CareerBuilder.\n",
    "\n",
    "The group created 500 computer models focused on specific job functions and locations. They taught each to recognize some 50,000 terms that showed up on past candidates’ resumes. The algorithms learned to assign little significance to skills that were common across IT applicants, such as the ability to write various computer codes, the people said.\n",
    "\n",
    "Instead, the technology favored candidates who described themselves using verbs more commonly found on male engineers’ resumes, such as “executed” and “captured,” one person said.\n",
    "\n",
    "Gender bias was not the only issue. Problems with the data that underpinned the models’ judgments meant that unqualified candidates were often recommended for all manner of jobs, the people said. With the technology returning results almost at random, Amazon shut down the project, they said.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4d1493",
   "metadata": {},
   "source": [
    "# Can an Algorithm Hire Better Than a Human?\n",
    "\n",
    "https://www.nytimes.com/2015/06/26/upshot/can-an-algorithm-hire-better-than-a-human.html\n",
    "\n",
    "\n",
    "“Every company vets its own way, by schools or companies on résumés,” said Sheeroy Desai, co-founder and chief executive of Gild, which makes software for the entire hiring process. “It can be predictive, but the problem is it is biased. They’re dismissing tons and tons of qualified people.”\n",
    "\n",
    "Yet some researchers say notions about chemistry and culture fit have led companies astray. That is because many interviewers take them to mean hiring people they’d like to hang out with.\n",
    "\n",
    "Another service, Textio, uses machine learning and language analysis to analyze job postings for companies like Starbucks and Barclays. Textio uncovered more than 25,000 phrases that indicate gender bias, said Kieran Snyder, its co-founder and chief executive. Language like “top-tier” and “aggressive” and sports or military analogies like “mission critical” decrease the proportion of women who apply for a job. Language like “partnerships” and “passion for learning” attract more women.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82966a4c",
   "metadata": {},
   "source": [
    "# When an Algorithm Helps Send You to Prison\n",
    "\n",
    "https://www.nytimes.com/2017/10/26/opinion/algorithm-compas-sentencing-bias.html?mtrref=www.google.com&gwh=647B25F30BE1F038621B9556991C3CB2&gwt=pay&assetType=opinion\n",
    "\n",
    "\n",
    "This article discusses the use of algorithms in the criminal justice system, focusing on the case of Eric Loomis in Wisconsin, who was sentenced partly based on a risk assessment algorithm called COMPAS. Here are the major points:\n",
    "\n",
    "    Eric Loomis was sentenced to prison based, in part, on a risk assessment algorithm called COMPAS after being arrested for non-violent offenses. This algorithm predicted his high risk of recidivism.\n",
    "\n",
    "    COMPAS is a proprietary algorithm, and its inner workings are not disclosed to the public. Judges consider its risk assessment score during sentencing.\n",
    "\n",
    "    Loomis challenged the use of COMPAS, arguing that it violated his due process rights, but the Wisconsin Supreme Court upheld its use, and the U.S. Supreme Court declined to hear his case.\n",
    "\n",
    "    The article questions the ethics of using a secretive algorithm to influence sentencing decisions, as it raises concerns about transparency and potential biases in the criminal justice system.\n",
    "\n",
    "    States are turning to algorithms in sentencing to reduce bias, but the article argues that this may not eliminate bias and can sometimes exacerbate it. Algorithms like COMPAS can reflect and reinforce societal biases present in the data used for training.\n",
    "\n",
    "    Algorithms lack the ability to provide individualized justice, unlike judges who can consider a defendant's unique circumstances, background, and potential for rehabilitation.\n",
    "\n",
    "    The article highlights that algorithms can operate on feedback loops, reinforcing initial determinations and potentially drifting away from fairness if not constantly retrained.\n",
    "\n",
    "    Transparency and accountability are crucial for the responsible use of algorithms in the criminal justice system. Examples of algorithms like the Public Safety Assessment in New Jersey are mentioned as successful when their functioning is openly reported and monitored to avoid biases.\n",
    "\n",
    "    The article concludes by emphasizing the need for human intervention and corrective action to ensure that algorithms do not perpetuate or exacerbate existing biases in the justice system. It argues that algorithms, while intelligent, are not inherently wise and must be managed with care and transparency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2196d6ff",
   "metadata": {},
   "source": [
    "# Benefits and Ethical Challenges in Data Science — COMPAS and Smart Meters\n",
    "\n",
    "https://towardsdatascience.com/benefits-and-ethical-challenges-in-data-science-compas-and-smart-meters-da549dacd7cd\n",
    "\n",
    "\n",
    "This essay explores the applications of Data Science in the context of COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) software used in US courts and domestic smart meters, focusing on their benefits and ethical challenges. Here are the main points from each section of the essay:\n",
    "\n",
    "Introduction:\n",
    "\n",
    "    Recent advances in data processing offer opportunities to improve lives and society.\n",
    "    Examples include smart meters for electricity consumption and the COMPAS software system.\n",
    "    Ethical questions arise when trading privacy for benefits or using algorithms in decision-making.\n",
    "\n",
    "Benefits and Ethical Challenges:\n",
    "\n",
    "    COMPAS is used in US courts to assess the risk of defendants re-offending.\n",
    "    It complements human judgment, offers efficiency, and reduces bias in decision-making.\n",
    "    Ethical challenges include unfair discrimination, reinforcing human biases, and a lack of transparency.\n",
    "    Smart meters benefit electricity suppliers for grid management and billing, while consumers gain transparency.\n",
    "    Ethical challenges in smart meters involve privacy, transparency, and consent.\n",
    "\n",
    "Critical Analysis:\n",
    "\n",
    "    The essay employs statistical analysis, trade-offs, individual and group privacy, and the ethics of practices.\n",
    "    COMPAS faces the challenge of balancing fairness, bias, and transparency.\n",
    "    Smart meters raise privacy concerns and issues related to data granularity and group privacy.\n",
    "    The essay advocates for a flexible policy framework to address the evolving ethical challenges.\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "    The essay concludes by asserting that consumers should not be pressured into trading privacy for monetary benefits, advocating for minimum privacy standards and open public discussions.\n",
    "    It emphasizes the need for flexible policy frameworks to address ethical challenges in Data Science applications.\n",
    "\n",
    "In summary, the essay explores the ethical dimensions of using data-driven technologies like COMPAS and smart meters and suggests the importance of balancing benefits with privacy and transparency while adapting to evolving ethical challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5ce414",
   "metadata": {},
   "source": [
    "# California city bans predictive policing in U.S. first\n",
    "\n",
    "https://www.reuters.com/article/us-usa-police-tech-trfn/california-city-bans-predictive-policing-in-u-s-first-idUSKBN23V2XC\n",
    "\n",
    "\n",
    "The city of Santa Cruz in California has become the first in the United States to ban predictive policing, a move hailed by digital rights experts as potentially setting a precedent. Predictive policing uses algorithms to analyze police records and direct officers to areas with higher predicted crime rates. Critics argue it reinforces racial biases in policing as it often leads to more police presence in low-income, ethnic minority neighborhoods. The mayor of Santa Cruz, Justin Cummings, cited concerns about the technology's racial bias and vowed to work on eliminating racism in policing. Boston's city council also recently voted to ban facial recognition technology. Activists continue to push for transparency and oversight in police use of technology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8df253",
   "metadata": {},
   "source": [
    "# What is facial recognition and how does it work?\n",
    "\n",
    "https://us.norton.com/blog/iot/how-facial-recognition-software-works\n",
    "\n",
    "Facial recognition is a biometric technology used to identify human faces through the analysis of facial features from photographs or videos, comparing this data with known faces in a database to find a match. It is widely used in various applications, including security, unlocking devices, marketing, and law enforcement. The technology works through the following steps:\n",
    "\n",
    "    Software analyzes images or videos containing an individual's face.\n",
    "    Facial features are mapped, including precise details like eye locations and facial differences.\n",
    "    The system compares the individual's facial signature with a database containing millions to billions of images.\n",
    "    The system determines if there is a match and may calculate an accuracy score or suggest alternatives.\n",
    "\n",
    "Facial recognition is prevalent in various places, such as airports, cellphones, classrooms, social media, businesses, and marketing campaigns. It offers advantages such as enhancing safety, identifying criminals, finding missing persons, and securing devices. However, it has limitations and drawbacks, including mistaken identity, reduced accuracy with age, racial and gender bias, susceptibility to tricks (e.g., masks or makeup), privacy concerns, security risks, and potential ownership issues.\n",
    "\n",
    "To protect privacy in a world with growing facial recognition technology, individuals can consider using facial recognition-blocking glasses, opting out of social media facial recognition systems, and securing their routers. While there are accuracy claims of nearly 100% for some systems, real-world accuracy rates vary. The technology has many potential uses and benefits but also poses challenges and risks, highlighting the importance of taking precautions to safeguard personal data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee7af70",
   "metadata": {},
   "source": [
    "# Facial Recognition Is Accurate, if You’re a White Guy\n",
    "\n",
    "https://www.nytimes.com/2018/02/09/technology/facial-recognition-race-artificial-intelligence.html?auth=login-email&login=email\n",
    "\n",
    "Facial recognition technology has shown significant disparities in accuracy based on the race and gender of individuals. A study conducted by Joy Buolamwini, a researcher at the M.I.T. Media Lab, revealed that the technology performs remarkably well in identifying white men, with a 99 percent accuracy rate. However, as the skin tone becomes darker and when identifying women, the error rates increase significantly, reaching nearly 35 percent for images of darker-skinned women. These disparities highlight the biases present in artificial intelligence (AI) systems, which rely on the data used to train them.\n",
    "\n",
    "The study raises concerns about the fairness and accountability of AI technology, particularly in contexts where facial recognition is used, such as hiring, lending, and law enforcement. The lack of diversity in the datasets used to train facial recognition systems, which often have a significant imbalance in terms of gender and race, contributes to these disparities.\n",
    "\n",
    "Facial recognition technology is being adopted in various applications, but it remains lightly regulated. There are concerns about the technology's potential for discrimination and the need for transparency and accountability in AI systems.\n",
    "\n",
    "Joy Buolamwini, who conducted the research, has become an advocate in the field of \"algorithmic accountability,\" seeking to address bias and discrimination in automated decision-making systems. She has founded the Algorithmic Justice League to raise awareness of these issues and is working on setting up standards for accountability and transparency in facial analysis software.\n",
    "\n",
    "This research highlights the importance of addressing biases and disparities in AI systems to ensure fairness and inclusivity in their applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644b8995",
   "metadata": {},
   "source": [
    "# Users Can Sue Facebook Over Facial Recognition Software, Court Rules\n",
    "\n",
    "\n",
    "https://www.npr.org/2019/08/08/749474600/users-can-sue-facebook-over-facial-recognition-software-court-rules\n",
    "\n",
    "\n",
    "The 9th Circuit U.S. Court of Appeals ruled that Facebook users in Illinois can sue the company over its use of facial recognition technology, allowing a class-action lawsuit to proceed. This marks the first decision by a U.S. appellate court to directly address privacy concerns related to facial recognition technology.\n",
    "\n",
    "The case revolves around Facebook users in Illinois who alleged that the company violated the state's Biometric Information Privacy Act by using their facial data without proper consent. Facebook argued that the users had not suffered any tangible harm, but the court ruled that intangible injuries, such as privacy intrusions, can still be considered concrete.\n",
    "\n",
    "Facebook's technology, particularly its \"tag suggestions\" feature, analyzes facial details in uploaded photos to create face templates. Once a face template is created, Facebook can use it to identify individuals in other photos and determine their presence at specific locations. The court noted that Facebook stores face templates on servers in various states.\n",
    "\n",
    "Facebook plans to request a full review of the decision by the three-judge panel. Privacy advocates have expressed concerns about the potential for facial recognition technology to be used for mass surveillance and violations of privacy rights.\n",
    "\n",
    "This ruling highlights the growing legal scrutiny and public concern surrounding the use of facial recognition technology by tech companies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cd5d10",
   "metadata": {},
   "source": [
    "#  Europe eyes stricter rules on facial recognition \n",
    "\n",
    "https://www.politico.eu/article/europe-facial-recognition-facebook-privacy-data-protection/\n",
    "\n",
    "A group of European privacy watchdogs is considering new guidelines that could tighten restrictions on the use of facial recognition technology in Europe. These guidelines would classify facial recognition data as \"biometric data,\" requiring explicit consent from individuals and imposing stricter privacy protections. This move comes amid concerns that the use of facial recognition without explicit consent violates the EU's General Data Protection Regulation (GDPR). If endorsed by national data protection authorities, these guidelines could curtail the use of facial recognition technology, impacting both governments and private companies. Facebook's use of facial recognition for tagging photos is also under scrutiny, and these guidelines may affect its practices. The guidelines will undergo a public consultation process before finalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa55fa6",
   "metadata": {},
   "source": [
    "# Real-Time Surveillance Will Test the British Tolerance for Cameras\n",
    "\n",
    "\n",
    "https://www.nytimes.com/2019/09/15/technology/britain-surveillance-privacy.html?nl=todaysheadlines&emc=edit_th_190916?campaign_id=2&instance_id=12318&segment_id=17053&user_id=16ecf3987642ea9959b524420f653d1e&regi_id=177540760916\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The use of facial recognition technology in Britain is testing the country's acceptance of surveillance. While Britain has a history of sacrificing privacy in the name of security, the introduction of real-time facial recognition technology is sparking controversy. Some members of Parliament have called for a moratorium on its use, and the country's top privacy regulator is investigating its use by both police and private businesses. Critics argue that the technology is an intrusion of privacy and has questionable accuracy, particularly for identifying non-white individuals. While Britain's use of facial recognition is not as widespread as in China, concerns are growing about its use in a democratic society."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6f6a50",
   "metadata": {},
   "source": [
    "# IBM will no longer offer, develop, or research facial recognition technology / IBM’s CEO says we should reevaluate selling the technology to law enforcement\n",
    "\n",
    "\n",
    "https://www.theverge.com/2020/6/8/21284683/ibm-no-longer-general-purpose-facial-recognition-analysis-software\n",
    "\n",
    "\n",
    "\n",
    "IBM has announced that it will no longer offer general-purpose facial recognition or analysis software and will cease developing or researching the technology. The decision comes as a response to concerns about the technology's potential for mass surveillance, racial profiling, and violations of human rights and freedoms. IBM CEO Arvind Krishna expressed the company's opposition to such uses of facial recognition technology and called for a national dialogue on how it should be employed by domestic law enforcement agencies. Facial recognition technology has faced criticism for issues related to racial bias and privacy violations. Researchers have revealed biases in commercial facial recognition systems, leading to calls for greater regulation and oversight. IBM's decision to halt its facial recognition efforts reflects growing concerns over the ethical and societal implications of the technology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f471e364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
