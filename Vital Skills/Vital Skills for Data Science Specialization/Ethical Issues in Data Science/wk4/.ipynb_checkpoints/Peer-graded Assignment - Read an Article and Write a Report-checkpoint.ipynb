{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84afca59",
   "metadata": {},
   "source": [
    "Find a recent media article that is relevant to the topic of algorithmic bias and is of interest to you. (Note: You may not use one of the assigned course readings for your report, although you may use one of the optional readings). Then write a report of 500-800 words that includes:\n",
    "\n",
    "    The reference information for the article (title, author, where and when published, link if online)\n",
    "\n",
    "    A summary of the content of the article\n",
    "\n",
    "    The issue(s) in data ethics that this article is relevant to\n",
    "\n",
    "    Your assessment of whether the issues discussed in the article are being handled and resolved properly, and what additional steps you feel should be considered.  Please refer to at least one ethical framework in making this assessment\n",
    "\n",
    "Paste your report into the provided textbox for submission.\n",
    "\n",
    "You will then review the reports of three of your peers and three peers will review your report. You will not receive a grade until reviews are complete.\n",
    "\n",
    "Your report should be in your own words. If you use any material that is directly copied from another author, you must place this material in quotation marks and properly cite it. Failure to do so constitutes plagiarism, which is grounds to fail not only the plagiarized assignment, but the course.\n",
    "\n",
    "Note: The peer grading is blind (that is, your name is not seen by others) so long as you do not put your name or other personally identifying information in the assignment title\n",
    "or the essay itself. Please do not put any self-identifying information in the essay title or the essay itself.\n",
    "\n",
    "GRADING RUBRIC:\n",
    "\n",
    "Does your peer clearly describe the topic of the article, in their own words? (150-300 words)\n",
    "\n",
    "◻Your peer correctly identifies the main topic of the article and clearly describes it in their own words. Their description of the topic is at least 150 words.(5 points)\n",
    "\n",
    "◻Your peer correctly identifies the main topic of the article in their own words. However, their description of the topic is either unclear or else is less than 150 words.(3 points).\n",
    "\n",
    "◻Your peer does not describe the topic of the article.(0 points)\n",
    "\n",
    "Does your peer clearly identify one or more ethically relevant issues raised by the article? (In order to be an “ethically relevant” issue, it must be possible for there to be two or more reasonable[1]but opposing viewpoints about whether the issue is being handled ethically or unethically). (150-300 words)\n",
    "\n",
    "◻Your peer clearly identifies at least one ethically relevant issue. In addition, they explain why there might be reasonable but opposing viewpoints about whether the issue is being handled ethically or unethically. Their description of the ethical issue is at least 150 words.(5 points)\n",
    "\n",
    "◻Your peer identifies at least one ethically relevant issue brought up by the article, but either 1) their description is too short, or 2) they do not satisfactorily explain why there might be reasonable but opposing viewpoints about whether the issue is being handled ethically or unethically.(3 points).\n",
    "\n",
    "◻Your peer either does not identify an issue brought up by the article, or the issue that they identify is not ethically relevant.(0 points)\n",
    "\n",
    "Does your peer take a stance on whether the relevant issue is being handled ethically or unethically, and do they defend their stance by correctly applying at least one of the three ethical frameworks discussed in class? (Note, for guidelines on what constitutes a “correct application” of an ethical framework, see the reading titled “Applying an Ethical Framework”) (200-400 words)\n",
    "\n",
    "◻Yes, completely. Moreover, this portion of their response is at least 200 words(5 points)\n",
    "\n",
    "◻Your peer takes a clear stance on whether the relevant issue is being handled ethically or unethically, and mentions at least one ethical framework discussed in class, but either 1) their analysis is too short, or 2) the way that they apply the ethical framework to the issue they are discussing demonstrates a misunderstanding of the ethical framework. (For more on this, see the “Applying an Ethical Framework” reading).(3 points)\n",
    "\n",
    "◻Your peer either does not take a stance on whether the relevant issue is being handled ethically or unethically, or else does not mention any of the three ethical frameworks discussed in class.(0 points)\n",
    "\n",
    "\n",
    "[1]To say that a viewpoint is “reasonable” is to say that it is the kind of view that someone who was thinking critically about ethics might actually hold for justifiable reasons. Consideringreasonabledifferences of ethical opinion helps to avoid what is known as a straw man fallacy:this happens when we characterize one or the other viewpoint as wrong based on the weakest arguments for it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c66b744",
   "metadata": {},
   "source": [
    "https://theweek.com/briefing/1023338/algorithm-ai-discrimination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f700ba",
   "metadata": {},
   "source": [
    "How content creators cope with discriminatory algorithms\n",
    "'AI has embedded our cultural biases and threatens to perpetuate discriminatory human behavior'\n",
    "\n",
    "The threat of bias in the latest wave of generative artificial intelligence may be in the spotlight lately, but social media algorithms already have discrimination problems. Some creators from marginalized communities have expressed frustration with how the algorithms appeared biased against them, robbing them of critical engagement.\n",
    "How do social media algorithms discriminate against some creators?\n",
    "\n",
    "While content that doesn't violate any explicit terms can't be outright banned, social media companies still have ways of suppressing the work of some creators. Shadow-bans are \"a form of online censorship where you're still allowed to speak, but hardly anyone gets to hear you,\" The Washington Post explained. Their content might not be removed, but some creators notice that engagement with their posts plummets outside of their immediate friends. \"Even more maddening, no one tells you it's happening,\" the Post added.\n",
    "\n",
    "Content creators have long decried the lack of transparency with shadow-bans. Late last year, the practice made headlines when Twitter owner Elon Musk released the Twitter Files, internal company documents intended to show how \"shadow-banning was being used to suppress conservative views,\" the Post said. \n",
    "\n",
    "Shadow-banning is a form of algorithmic bias that disproportionately affects specific demographics because the \"unconscious biases of the developers are embedded in the systems they create,\" Annie Brown wrote for Forbes. Additionally, \"algorithms are trained by data gathered from human history, a history replete with violence, inequity, bias and cruelty,\" Brown posited. Shadow-bans are \"just one symptom of the inherent bias, racism and marginalization algorithms have detected and AI has co-opted,\" Brown opined. \"Seen this way, AI, under the guise of observation and platform moderation, has embedded our cultural biases and threatens to perpetuate discriminatory human behavior.\"\n",
    "Who has accused social media companies of algorithmic bias? \n",
    "\n",
    "Black creators have been speaking out about their content being suppressed since TikTok was accused of suppressing the content of Black creators during the George Floyd protests in 2020. The company later released a statement apologizing for a \"technical glitch\" that made it temporarily appear that \"posts uploaded using #BlackLivesMatter and #GeorgeFloyd would receive 0 views.\" Some creators alleged that their engagement still went down after posting content with those hashtags.  \n",
    "\n",
    "The following year, creators pointed out that terms like \"Black Lives Matter'' and \"Black people\" were flagged as inappropriate by the automated moderation system. In contrast, words like \"white supremacy\" or \"white success\" did not trigger a warning. Black dancers and choreographers also alleged that TikTok's recommendation algorithm prioritized white creators who copied their dances without giving them credit. This eventually led them to have a content strike on the platform that year. \n",
    "\n",
    "LGBTQ+ content creators have also raised concerns about their posts being taken down with little to no explanation, a practice labeled as \"the digital closet\"  by researcher and author Alexander Monea in his book of the same name. For his book about the overpolicing of LGBTQ-centered online spaces, Monea spent two years looking through data and collecting anecdotes from LGBTQ+ social media users who reported \"being censored, silenced or demonetized,\" ABC News explained. \n",
    "\n",
    "\"Once the internet is largely controlled by a very few companies that all use an advertising model to drive their revenue, what you get is an overpoliced sort of internet space,\" Monea told ABC's \"Perspective\" podcast.\n",
    "\n",
    "When Tumblr adopted an adult content ban in 2018, reports that the ban disproportionately affected LGBTQ+ users led to an investigation by New York City's Commission on Human Rights. In an interview, Monea said the \"automated content moderation algorithms that Tumblr implemented to help institute its new ban\" was \"comically inept but with tragic consequences.\" Many LGBTQ+ users lost all of their content \"with no redress and no way to recover their lost content or user base,\" Monea added. \n",
    "\n",
    "In 2022, Tumblr reached a settlement with New York City's Commission on Human Rights after an investigation was launched into the allegations of discrimination against LGBTQ+ users. The settlement required the platform to \"revise its user appeals process and train its human moderators on diversity and inclusion issues, as well as review thousands of old cases and hire an expert to look for potential bias in its moderation algorithms,\" The Verge summarized. \n",
    "How do creators cope with algorithmic bias? \n",
    "\n",
    "To avoid the looming threat of shadow-banning, some content creators have taken to using workarounds \"such as not using certain images, keywords or hashtags or by using a coded language known as algospeak,\" the Post explained. \n",
    "\n",
    "\"There's a line we have to toe; it's an unending battle of saying something and trying to get the message across without directly saying it,\" TikTok creator Sean Szolek-VanValkenburgh told the Post. \"It disproportionately affects the LGBTQIA community and the BIPOC community because we're the people creating that verbiage and coming up with the colloquiums.\"\n",
    "\n",
    "Some creators have attempted to fight against social media companies accused of discriminatory moderation with lawsuits. However, \"bias allegations against social media platforms have rarely succeeded in court,\" The Verge noted. YouTube won two lawsuits from LGBTQ+ and Black video creators who alleged algorithmic discrimination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbd1702",
   "metadata": {},
   "source": [
    "Theara Coleman\n",
    "\n",
    "May 16, 2023\n",
    "\n",
    "How content creators cope with discriminatory algorithms\n",
    "\n",
    "https://theweek.com/briefing/1023338/algorithm-ai-discrimination\n",
    "\n",
    "theweek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1587ce",
   "metadata": {},
   "source": [
    "Theara Coleman\n",
    "May 16, 2023\n",
    "How content creators cope with discriminatory algorithms\n",
    "https://theweek.com/briefing/1023338/algorithm-ai-discrimination\n",
    "theweek\n",
    "\n",
    "\n",
    "Summary:\n",
    "The article discusses the issue of algorithmic bias and discrimination faced by content creators on social media platforms. It highlights the practice of shadow-banning, where creators' content is suppressed, leading to a significant decline in engagement. The unconscious biases of developers and the training of algorithms on historical data contribute to these discriminatory outcomes. The article mentions cases where social media companies have been accused of algorithmic bias against marginalized communities, such as Black creators and LGBTQ+ content creators. Some creators have resorted to workarounds and coded language to navigate the biased algorithms. Legal action against social media platforms for discriminatory moderation has had limited success.\n",
    "\n",
    "Issues in Data Ethics:\n",
    "\n",
    "- Algorithmic Bias: The article raises concerns about the inherent biases present in algorithms due to the historical data they are trained on. These biases can lead to discriminatory outcomes, affecting specific demographics and perpetuating existing inequalities.\n",
    "\n",
    "- Lack of Transparency: The lack of transparency in how social media algorithms operate, including shadow-banning, raises ethical concerns. Content creators have expressed frustration over the opacity surrounding algorithmic decisions, as they are often unaware of the reasons behind their reduced visibility or engagement.\n",
    "\n",
    "Assessment of Handling and Resolution:\n",
    "The issues discussed in the article highlight the ongoing challenges of algorithmic bias and discrimination faced by content creators. While some social media platforms have acknowledged and addressed specific instances of bias, there is still significant room for improvement.\n",
    "\n",
    "From an ethical standpoint, it is crucial for social media companies to adopt a more transparent approach to algorithmic decision-making. This can include providing clear guidelines and explanations for content moderation actions, as well as offering avenues for appeal and redressal when unjustified biases are identified.\n",
    "\n",
    "One ethical framework that can be applied to assess the handling of these issues is the principles of fairness and justice. Social media platforms should ensure that their algorithms are designed and continuously monitored to minimize biases and ensure equitable treatment of all creators. This can be achieved through diverse development teams, algorithmic auditing, and incorporating feedback mechanisms from affected communities.\n",
    "\n",
    "In adition, platforms should proactively engage with marginalized communities, seeking their input and perspectives when developing and refining algorithms. Collaborative efforts, such as partnerships with external organizations or establishing advisory boards, can help address bias and discrimination concerns more effectively.\n",
    "\n",
    "Also, ongoing research and innovation in algorithmic fairness are essential. Social media companies should invest in developing and deploying algorithms that prioritize fairness and inclusivity, and regularly evaluate their impact to identify and rectify biases. fostering an open dialogue with content creators and communities affected by algorithmic bias can aid in building trust and improving accountability. By actively addressing concerns and implementing feedback, social media platforms can work towards creating a more inclusive and fair digital landscape for all users.\n",
    "\n",
    "In summary, addressing algorithmic bias requires a multi-faceted approach that combines transparency, inclusivity, and continuous improvement. By prioritizing fairness and justice, social media platforms can create an environment that empowers content creators from all backgrounds and fosters a more equitable online ecosystem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc43397e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41542b7f",
   "metadata": {},
   "source": [
    "Title: Eliminating Algorithmic Bias Is Just the Beginning of Equitable AI\n",
    "\n",
    "https://hbr.org/2023/09/eliminating-algorithmic-bias-is-just-the-beginning-of-equitable-ai\n",
    "\n",
    "\n",
    "by: Simon Friis and James Riley \n",
    "\n",
    "September 29, 2023\n",
    "\n",
    "HBR Staff/Imansyah Muhamad Putera/Unsplash\n",
    "\n",
    "\n",
    "Summary. When it comes to artificial intelligence and inequality, algorithmic bias rightly receives a lot of attention. But it’s just one way that AI can lead to inequitable outcomes. To truly create equitable AI, we need to consider three forces through which it might make society more or less equal: technological forces, supply-side forces, and demand-side forces. The last of these is particularly underemphasized. The use of AI in a product can change how much customers value it — for example, patients who put less stock in an algorithmic diagnosis — which in turn can affect how that product is used and how those working alongside it are compensated.\n",
    "\n",
    "\n",
    "\n",
    "From automating mundane tasks to pioneering breakthroughs in healthcare, artificial intelligence is revolutionizing the way we live and work, promising immense potential for productivity gains and innovation. Yet, it has become increasingly apparent that the promises of AI aren’t distributed equally — it risks exacerbating social and economic disparities, particularly across demographic characteristics such as race.\n",
    "\n",
    "Business and government leaders are being called on to ensure the benefits of AI-driven advancements are accessible to all. Yet it seems that for each passing day there is some new way in which AI creates inequality, resulting in a reactive patchwork of solutions — or often no response at all. If we want to effectively address AI-driven inequality, we will need a proactive, holistic approach.\n",
    "\n",
    "If policymakers and business leaders hope to make AI more equitable, they should start by recognizing three forces through which AI can increase inequality. We recommend a straightforward, macro-level framework that encompasses these three forces but centers the intricate social mechanisms through which AI creates and perpetuates inequality. This framework boasts dual benefits. First, its versatility ensures applicability across diverse contexts, from manufacturing to healthcare to art. Second, it illuminates the often-overlooked, interdependent ways AI alters demand for goods and services, a significant pathway by which AI propagates inequality.\n",
    "\n",
    "Our framework consists of three interdependent forces through which AI creates inequality: technological forces, supply-side forces, and demand-side forces.\n",
    "Technological forces: Algorithmic bias\n",
    "\n",
    "Algorithmic bias occurs when algorithms make decisions that systematically disadvantage certain groups of people. It can have disastrous consequences when applied to key areas such as healthcare, criminal justice, and credit scoring. Scientists investigating a widely used healthcare algorithm found that it severely underestimated the needs of Black patients, leading to significantly less care. This is not just unfair, but profoundly harmful. Algorithmic bias often occurs because certain populations are underrepresented in the data used to train AI algorithms or because pre-existing societal prejudices are baked into the data itself.\n",
    "\n",
    "While minimizing algorithmic bias is an important piece of the puzzle, unfortunately it is not sufficient for ensuring equitable outcomes. Complex social processes and market forces lurk beneath the surface, giving rise to a landscape of winners and losers that cannot be explained by algorithmic bias alone. To fully understand this uneven landscape, we need to understand how AI shapes the supply and demand for goods and services in ways that perpetuate and even create inequality.\n",
    "Supply-side forces: Automation and augmentation\n",
    "\n",
    "AI often lowers the costs of supplying certain goods and services by automating and augmenting human labor. As research by economists like Erik Brynjolfsson and Daniel Rock reveals, some jobs are more likely to be automated or augmented by AI than others. A telling analysis by the Brookings Institution found that “Black and Hispanic workers … are over-represented in jobs with a high risk of being eliminated or significantly changed by automation.” This isn’t because the algorithms involved are biased, but because some jobs consist of tasks that are easier (or more financially lucrative) to automate such that investment in AI is a strategic advantage. But because people of color are often concentrated in those very jobs, automation and augmentation of work through AI and digital transformations more broadly has the potential to create inequality along demographic lines.\n",
    "Demand-side forces: Audience (e)valuations\n",
    "\n",
    "The integration of AI in professions, products, or services can affect how people value them. In short, AI alters demand-side dynamics, too.\n",
    "\n",
    "Suppose you discover your doctor uses AI tools for diagnosis or treatment. Would that influence your decision to see them? If so, you are not alone. A recent poll found that 60% of U.S. adults would be uncomfortable with their healthcare provider relying on AI to treat and diagnose diseases. In economic terms, they may have lower demand for services that incorporate AI.\n",
    "Why AI-augmentation can lower demand\n",
    "\n",
    "Our recent research sheds light on why AI-augmentation can lower demand for a variety of goods and services. We found that people often perceive the value and expertise of professionals to be lower when they advertise AI-augmented services. This penalty for AI-augmentation occurred for services as diverse as coding, graphic design, and copyediting.\n",
    "\n",
    "However, we also found that people are divided in their perceptions of AI-augmented labor. In the survey we conducted, 41% of respondents were what we call “AI Alarmists” — people who expressed reservations and concerns about AI’s role in the workplace. Meanwhile, 31% were “AI Advocates,” who wholeheartedly champion the integration of AI in the labor force. The remaining 28% were “AI Agnostics,” those who sit on the fence, recognizing both potential benefits and pitfalls. This diversity of views underlines the absence of a clear, unified mental model on the value of AI-augmented labor. While these results are based on a relatively small online survey, and do not capture how all of society views AI, they do point to distinct differences between individuals’ social (e)valuations of the uses and users of AI and how this informs their demand for goods and services — which is at the heart of what we plan to explore in further studies.\n",
    "How demand-side factors perpetuate inequality\n",
    "\n",
    "Despite its significance, this perspective — how audiences perceive and value AI-augmented labor — is often glossed over in the broader dialogue about AI and inequality. This demand-side analysis is an important part of understanding the winners and losers of AI, and how it can perpetuate inequality.\n",
    "\n",
    "That’s especially true in cases where peoples’ perceived value of AI intersects with bias against marginalized groups. For example, the expertise of professionals from dominant groups is typically assumed, while equally qualified professionals from traditionally marginalized groups often face skepticism about their expertise. In the example above, people are skeptical of doctors’ relying on AI — but that distrust may not play out in the same ways across professionals with varying backgrounds. Doctors from marginalized backgrounds, who already face skepticism from patients, are likely to bear the brunt of this loss of confidence caused by AI.\n",
    "\n",
    "While efforts are already underway to address algorithmic bias as well as the effects of automation and augmentation, it is less clear how to address audience’s biased valuations of historically disadvantaged groups. But there’s hope.\n",
    "Aligning social and market forces for an equitable AI future\n",
    "\n",
    "To truly foster an equitable AI future, we must recognize, understand, and address all three forces. These forces, while distinct, are tightly intertwined, and fluctuations in one reverberate throughout the others.\n",
    "\n",
    "To see how this plays out, consider a scenario where a doctor refrains from using AI tools to avoid alienating patients, even if the technology improves healthcare delivery. This reluctance not only affects the doctor and their practice but deprives their patients of AI’s potential advantages such as early detection during cancer screenings. And if this doctor serves diverse communities this might also result in exacerbating the underrepresentation of those communities and their health factors in AI training datasets. Consequently, the AI tools become less attuned to the specific needs of these communities, perpetuating a cycle of disparity. In this way, a detrimental feedback loop can take shape.\n",
    "\n",
    "The metaphor of a tripod is apt: a deficiency in just one leg directly impacts the stability of the entire structure, which impacts the ability to adjust angles and perspectives, and inevitably its value to its users.\n",
    "\n",
    "To prevent the negative feedback loop described above, we would do well to look to frameworks that enable us to develop mental models of AI-augmented labor that promote equitable gains. For example, platforms that provide AI-generated products and services need to educate buyers on AI-augmentation and the unique skills required for working effectively with AI tools. One essential component is to emphasize that AI augments, rather than supplants, human expertise.\n",
    "\n",
    "Though rectifying algorithmic biases and mitigating the effects of automation are indispensable, they are not enough. To usher in an era where the adoption of AI acts as a lifting and equalizing force, collaboration between stakeholders will be key. Industries, governments, and scholars must come together through thought partnerships and leadership to forge new strategies that prioritize human-centric and equitable gains from AI. Embracing such initiatives will ensure a smoother, more inclusive, and stable transition into our AI-augmented future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7abd06",
   "metadata": {},
   "source": [
    "1. clearly describe the topic of the article in own words )150-300 words\n",
    "\n",
    "2. clearly identify one or more ethically relevant issues raised by the artical (In order to be an 'ethically relevant' issue, it must be possible for there to be two or more reasonable but opposing viewpoints about whether the issue is being handled ethically or unethically (150-300 words)\n",
    "\n",
    "3. take a stance on whether the relevant issue is being handled ethically or unethically, and defend your stance by correctly applying at least one of the three ethical frameworks (Deontology, Virtue ethics, Utilitarianism) (200-400words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2cc093",
   "metadata": {},
   "source": [
    "1.  244 words.\n",
    "\n",
    "The article \"Eliminating Algorithmic Bias Is Just the Beginning of Equitable AI\" explores the multifaceted issue of artificial intelligence and its impact on societal equity. While algorithmic bias, a well-documented concern, is a focal point, the article delves deeper into three interrelated forces through which AI can either exacerbate or mitigate inequality.\n",
    "\n",
    "Firstly, it discusses \"technological forces,\" emphasizing the need to address algorithmic bias that leads to unfair outcomes, particularly in critical areas like healthcare and criminal justice. However, it goes beyond algorithmic bias, highlighting the intricate social processes that contribute to unequal distribution.\n",
    "\n",
    "Then, the article examines \"supply-side forces,\" where AI automation and augmentation of labor can have both positive and negative consequences. It points out that certain jobs are more susceptible to being automated or augmented, potentially leading to disparities, particularly among demographic groups.\n",
    "\n",
    "Lastly, the article introduces the often-overlooked \"demand-side forces.\" It explains how the integration of AI in products or services can influence how people perceive and value them. This factor can significantly affect the demand for AI-incorporated goods and services, contributing to inequality, especially when biased perceptions intersect with marginalized groups.\n",
    "\n",
    "Overall, the article calls for a holistic approach to tackle AI-driven inequality by recognizing and addressing these three forces. It emphasizes the importance of collaboration between industries, governments, and scholars to create strategies that prioritize equitable gains from AI, ultimately promoting a more inclusive and stable transition into an AI-augmented future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34da3d1a",
   "metadata": {},
   "source": [
    "2. 304 words\n",
    "\n",
    "The article highlights several ethically relevant issues in the context of AI and equitable outcomes, among them are: Algorithmic Bias and Fairness,  Impact on Employment and Economic Disparities, Perception of AI-Augmented Services, and Equity in AI Development and Access.\n",
    "\n",
    "    Algorithmic Bias and Fairness; One prominent ethical concern is algorithmic bias, where AI systems make decisions that systematically disadvantage certain groups. The article acknowledges the harm caused when algorithms in healthcare, criminal justice, or credit scoring exhibit bias. The ethical question here is how to ensure fairness in AI decision-making. Some argue that it's unethical for AI systems to perpetuate historical biases, while others might argue that algorithms should focus on objective outcomes, even if it means some groups are disproportionately affected.\n",
    "\n",
    "    Impact on Employment and Economic Disparities, The article discusses how AI can automate or augment certain jobs, potentially leading to job displacement and economic disparities, particularly for marginalized groups. The ethical dilemma revolves around how to address the potential inequalities arising from AI-driven automation. Some may argue that it's unethical not to provide safeguards or opportunities for those affected, while others might argue that technological progress should not be hindered, even if it results in job changes or losses.\n",
    "\n",
    "    Perception of AI-Augmented Services, The article introduces the concept of \"demand-side forces,\" where the integration of AI can influence how people perceive and value services. The ethical issue is whether it's ethical for individuals to perceive AI-augmented professionals as less capable or valuable, potentially affecting their livelihoods. On one hand, some may argue that such perceptions are unfair and lead to discrimination, while others may argue that it's natural for people to prefer human expertise over AI.\n",
    "\n",
    "    Equity in AI Development and Access, There's an underlying ethical concern about who benefits from AI advancements and who gets left behind. The article calls for a proactive, holistic approach to ensure AI benefits are accessible to all. The ethical question is how to ensure equitable access to AI technologies and benefits. Some argue that it's unethical for AI development to disproportionately benefit certain groups, while others might prioritize technological advancement over equal distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b68a9e",
   "metadata": {},
   "source": [
    "3. 257 words\n",
    "\n",
    "The ethically relevant issue of algorithmic bias in AI decision-making is currently being handled unethically, primarily from a deontological ethics standpoint. Deontology is an ethical framework that emphasizes the moral duty or principle as the basis for determining whether an action is right or wrong, regardless of the consequences.\n",
    "\n",
    "From a deontological perspective, algorithmic bias is unethical because it violates the fundamental principle of treating all individuals with equal respect and dignity. When AI systems perpetuate biases against certain groups, such as underestimating the healthcare needs of Black patients, it goes against the moral duty to provide equal and fair treatment to all individuals, regardless of their race or background.\n",
    "\n",
    "Furthermore, deontology places importance on following universal moral principles. In the context of AI, this means that data scientist and users of AI systems have a duty to ensure that these systems do not discriminate or harm certain groups. When algorithmic bias leads to unequal outcomes, it violates this universal moral principle.\n",
    "\n",
    "However, it's essential to acknowledge that addressing algorithmic bias is a complex challenge, and data scientist may not intentionally design biased algorithms. Nonetheless, from a deontological perspective, the unintentional perpetuation of bias is still considered unethical because it results in unequal treatment.\n",
    "\n",
    "Additionally, when it comes to AI ethics, deontology aligns with the concept of fairness and justice. Fairness is a fundamental principle in AI ethics, and addressing algorithmic bias is a crucial step in achieving fairness in AI systems. Therefore, not adequately addressing algorithmic bias would be seen as unethical from a deontological standpoint, as it goes against the duty to uphold fairness and justice in AI.\n",
    "\n",
    "In conclusion, the relevant issue of algorithmic bias in AI decision-making is currently being handled unethically, primarily from a deontological ethics perspective. Addressing this issue is not merely a matter of achieving better consequences; it is a moral duty to ensure equal treatment and uphold universal principles of fairness and justice in AI systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a7b587",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
