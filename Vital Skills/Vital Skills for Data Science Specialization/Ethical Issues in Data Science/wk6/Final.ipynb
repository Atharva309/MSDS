{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e68cd8e5",
   "metadata": {},
   "source": [
    "Ethical Frameworks\n",
    "- Kantianism / Deontology - humans possess the ability to reason and understand universal moral laws that they can apply in all situations.\n",
    "- Virtue Ethics-  urges people to live a moral life by cultivating virtuous habits.\n",
    "- Utilitarianism - the doctrine that actions are right if they are useful or for the benefit of a majority.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d22a24",
   "metadata": {},
   "source": [
    "DTSA 5303\tEthical Issues in Data Science\t\n",
    "- Project\t35%\t\n",
    "- Written reflections on earlier prompt responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93ac84b",
   "metadata": {},
   "source": [
    "# About the Final Assignment\n",
    "The Final Submission for the credit course is a compilation of the discussion posts and accounts for 35% of total grade.  Remember, if you post discussions in the non-credit course, you will lose access to those posts if/when you switch to the credit course.From the Submission Instructions:\n",
    "For this final assignment, you will revisit your answer to a specific discussion board question (or portion of it) from each of the five weeks of this course. In each case, you may:\n",
    "\n",
    "a) repeat your answer exactly as you gave it originally, or\n",
    "\n",
    "b) revise and expand upon your original answer, or\n",
    "\n",
    "c) start anew in answering the question. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e40a16f",
   "metadata": {},
   "source": [
    "The final consists of 5 individual smaller reports (2-3 paragraphs) where each one refers to one of the discussion post questions of each week. It is either the same question or part of it. This means if you already did all these posts you can reuse them for the final. In my case I expanded a little bit on each one to get the required length. In the end you should not have to spend a lot of time on the final if you already did the work before. (edited) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475784b3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc6f89b-5d7a-44f5-aefe-0976b407cb9b",
   "metadata": {},
   "source": [
    "# Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa898d90",
   "metadata": {},
   "source": [
    "# Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06998125",
   "metadata": {},
   "source": [
    "# Analyze a Case Study\n",
    "\n",
    "Choose a case study from the ones provided with the ACM Code of Professional Ethics, at https://www.acm.org/code-of-ethics/case-studies\n",
    "\n",
    "Write and post (on the discussion board) an essay of 500-800 words that:\n",
    "\n",
    "- Briefly analyzes this case from the perspectives of each of the ethical frameworks we have studied: Kantianism/Deontology; Virtue Ethics; Utilitarianism.   For each, do you feel that framework suggests whether the situation is ethical or not?  Or do you feel it does not lead to a conclusive determination?   Whichever you feel (ethical / inconclusive / not-ethical) please state briefly what you feel that framework leads to that conclusion.\n",
    "\n",
    "- From what you have seen so far, which ethical frameworks appeal to you most, and least, and why?   \n",
    "\n",
    "Please note that there are rarely right and wrong answers to questions framed for short essay responses in this course.   Rather, the goal is to think issues through from a variety of perspectives. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e293639-e5e1-482a-9c60-06a1e3fd5329",
   "metadata": {},
   "source": [
    "Corazón, a medical technology startup, faces an ethical dilemma regarding a potential vulnerability in its implantable heart health monitoring device. This essay will analyze the case from the perspectives of three ethical frameworks: Kantianism/Deontology, Virtue Ethics, and Utilitarianism.\n",
    "\n",
    "### Kantianism/Deontology:\n",
    "\n",
    "Kantianism emphasizes the importance of acting in accordance with moral duties and principles. In Corazón’s case, the company seems to align with Kantian ethics in several aspects. The commitment to patient security and well-being, as evidenced by their adherence to regulatory standards and encryption practices, reflects a duty-driven approach. Additionally, their open bug bounty program shows a commitment to transparency and the duty to continually improve their product's security.\n",
    "\n",
    "However, the use of a hard-coded initialization value in the implant raises concerns. From a deontological perspective, the potential risks associated with this design choice must be thoroughly evaluated, and if necessary, steps should be taken to rectify it. The duty to prioritize patient safety may necessitate a reevaluation of this particular aspect of the device.\n",
    "\n",
    "### Virtue Ethics:\n",
    "\n",
    "Virtue ethics focuses on the development of good character traits. Corazón demonstrates virtues such as responsibility, transparency, and a commitment to societal well-being. Their engagement with charities to provide free or reduced access to those in need reflects virtues of compassion and social responsibility.\n",
    "\n",
    "From a virtue ethics standpoint, Corazón’s actions seem ethical as they exhibit virtues that contribute positively to both individuals and society. However, virtues alone may not provide a complete ethical assessment. The potential risks associated with the hard-coded value should still be thoroughly evaluated and addressed.\n",
    "\n",
    "### Utilitarianism:\n",
    "\n",
    "Utilitarianism assesses the ethicality of actions based on their consequences and the overall happiness or well-being they produce. Corazón’s products contribute to societal well-being by improving heart health monitoring accessibility. Their charity collaborations further extend the positive impact to individuals living below the poverty line.\n",
    "\n",
    "In terms of the vulnerability discovered by the researcher, a utilitarian perspective would consider the overall consequences. If the risk of harm is indeed negligible, as concluded by the researcher, and the benefits of the implant in terms of health monitoring and charity work are substantial, the situation might be deemed ethical from a utilitarian standpoint.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "Corazón’s case presents a complex ethical scenario with elements aligning with and challenging different ethical frameworks. While their commitment to patient well-being, adherence to regulations, and transparency reflect ethical virtues and duties, the potential risk associated with the hard-coded value requires careful consideration.\n",
    "\n",
    "In conclusion, the case does not provide a conclusive determination through any single ethical framework. The company must conduct a comprehensive risk analysis, considering potential harm and benefits, to make an informed ethical decision. Balancing duty, virtue, and utility will be crucial for Corazón to navigate this ethical dilemma successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550ec9b6-107a-4f85-aa28-9ff55b6c7b63",
   "metadata": {},
   "source": [
    "### From what you have seen so far, which ethical frameworks appeal to you most, and least, and why?   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991c0feb-8f53-46fa-8069-cd22ab958e83",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef5d395",
   "metadata": {},
   "source": [
    "https://ethics.acm.org/code-of-ethics/using-the-code/case-automated-active-response-weaponry/\n",
    "\n",
    "# Case: Automated Active Response Weaponry\n",
    "\n",
    "## Using the Code: Automated Active Response Weaponry\n",
    "\n",
    "Q Industries is an international defense contractor that specializes in autonomous vehicles. Q’s early work focused on passive systems, such as bomb-defusing robots and crowd-monitoring drones. As an early pioneer in this area, Q established itself as a vendor of choice for both military and law enforcement applications. Q’s products have been deployed in a variety of settings, including conflict zones and non-violent protests.\n",
    "\n",
    "In recent years, Q has suffered a number of losses, as protestors and other individuals have physically attacked the vehicles with rocks, guns, and other weapons. To reduce this problem, Q has begun to experiment with automated active responses. Q’s first approach was to employ facial recognition algorithms to record those present and to detect individuals who may pose a threat. This approach was shortly followed by automated non-lethal responses, such as tear gas, pepper spray, or acoustic weapons, to incapacitate threatening individuals.\n",
    "\n",
    "Q has recently been approached—in secret meetings—by multiple governments to expand this response to include lethal responses of varying scales. These capabilities range from targeted shooting of known individuals to releasing small-scale explosives. When Q leadership agreed to pursue these capabilities, several of Q’s original engineers resigned in protest. Some of these engineers had previously expressed concern that the non-lethal responses had inadequate protections against tampering, such as replacing tear gas with a lethal poison. Knowing that these individuals were planning to speak out publicly, Q sued them for violating the terms of their confidentiality employment agreement.\n",
    "\n",
    "## Analysis\n",
    "\n",
    "Principle 1.2 states that systems designed with the intention of causing harm must be ethically justified and must minimize unintended harm. Q’s evolving work shows a gradual process toward violating this principle. Q’s first automated response employed machine learning in a way that could be used to suppress free speech and association, which are fundamental human rights (Principle 1.1). By failing to account for this risk, Q failed to adhere to Principle 2.5’s mandate of extraordinary care regarding risks in machine learning systems. Furthermore, these tools could be used by governments that do not respect the values expressed in Principle 1.4, allowing discrimination and other abuses. This technology also failed to respect the privacy of other individuals who may be innocent bystanders (Principle 1.6). Additionally, the harm induced by the non-lethal weapons was not minimized, as these weapons would subject innocent individuals to pain or death without due process. Thus, the harm made possible here is not ethically justified and violates Principle 1.2.\n",
    "\n",
    "The engineers who resigned and decided to speak out would be justified in doing so, based on Principles 1.2, 1.7, and 2.7. The harm that Q’s systems made possible violated several aspects of the ethical obligations of the Code. Thus, the engineers were right to foster public awareness of these risks, and breaking their confidentiality agreements would be ethically justified. At the same time, Q’s lawsuit is justified and the engineers must accept the consequences for breaking these agreements, per Principle 2.3.\n",
    "\n",
    "Overall, Q’s leadership failed to respect the Code’s overarching emphasis on the public good. The Preamble introduces this emphasis by stating that public good is the paramount consideration. Principle 3.1 reiterates this point, declaring that people should always be the central concern. Q’s leadership failed to adhere to this key tenet that underlies the other principles of the Code.\n",
    "\n",
    "These cases studies are designed for educational purposes to illustrate how to apply the Code to analyze complex situations. All names, businesses, places, events, and incidents are fictitious and are not intended to refer to actual entities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898af16c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0720ead0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Week 2 (X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e641520",
   "metadata": {},
   "source": [
    "# Recommender Systems\n",
    "\n",
    "Select one ethical framework (your choice) and write on the discussion board whether you think this framework supports or does not support the use of recommender systems to feed you more YouTube content (songs) and with that, more advertisements.   If you prefer to comment on the case where recommender systems suggest political content as discussed by Tufecki, rather than on musical content that is fine too.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2602ae70",
   "metadata": {},
   "source": [
    "Virtue Ethics is a normative ethical theory that focuses on the development of virtuous character traits and emphasizes the importance of moral virtues in guiding behavior. Virtue Ethics is often associated with Aristotle, who argued that virtuous actions arise from a virtuous character.\n",
    "\n",
    "When it comes to the use of recommender systems on platforms like YouTube, Virtue Ethics does not inherently support or oppose them. Instead, Virtue Ethics provides a framework for evaluating the morality of actions based on virtues such as honesty, integrity, fairness, and benevolence.\n",
    "\n",
    "In the context of recommender systems on YouTube, the ethical evaluation would depend on how these systems are designed and implemented. If the recommender system operates with transparency, honesty, and respects the well-being of users, it could align with Virtue Ethics. For example, a system that recommends content to enrich a user's experience, introducing them to new music and relevant advertisements without manipulating or exploiting them, may be seen as virtuous.\n",
    "\n",
    "On the other hand, if the recommender system is designed to exploit users by promoting addictive content or manipulating their preferences for financial gain, it would be considered ethically problematic from a Virtue Ethics perspective. This is because such actions would be inconsistent with virtues like honesty, integrity, and benevolence.\n",
    "\n",
    "In conclusion, Virtue Ethics does not provide a clear-cut answer regarding the use of recommender systems on platforms like YouTube. The ethical evaluation depends on the virtues embodied in the design and implementation of these systems. Transparency, honesty, and a focus on user well-being are likely to align with Virtue Ethics, while manipulative practices may be viewed as ethically objectionable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5073b46",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcf3eaa",
   "metadata": {},
   "source": [
    "# The Right to Be Forgotten\n",
    "\n",
    "Comment on the following:\n",
    "\n",
    "- Do you feel there should be a Right to be Forgotten in your current nation of residence?\n",
    "\n",
    "- Do you think this policy should be universal, or vary in different countries / regions?\n",
    "\n",
    "- What is the basis for your opinion?  (Ethical framework we’ve studied, or something else)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4d3fa9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0443e446-92b7-4e8d-8ebc-90e742a929f1",
   "metadata": {},
   "source": [
    "The concept of the Right to Be Forgotten is a complex and debated issue that involves balancing privacy rights against the public's right to access information. Different individuals may hold varying opinions on this matter based on their cultural, legal, and ethical perspectives.\n",
    "\n",
    "As for whether there should be a Right to Be Forgotten in one's current nation of residence, opinions may differ. Some may argue in favor of it, emphasizing the importance of personal privacy and the right to move on from past mistakes or irrelevant information. Others may argue against it, citing concerns about potential censorship, historical record preservation, and the public's right to know.\n",
    "\n",
    "The question of whether this policy should be universal or vary in different countries or regions adds another layer of complexity. Cultural and legal differences across nations can significantly impact how such a right is interpreted and implemented. Some might argue for a universal standard to ensure consistency and fairness, while others may advocate for flexibility to accommodate diverse cultural norms and legal frameworks.\n",
    "\n",
    "The basis for one's opinion on the Right to Be Forgotten can stem from various ethical frameworks. For instance, those who prioritize individual autonomy and privacy may support the concept, viewing it as a means to protect individuals from the long-lasting consequences of their past actions. On the other hand, those who emphasize the importance of transparency and the public's right to access information may be more critical of such a right, concerned about potential abuse and selective memory erasure.\n",
    "\n",
    "In essence, the debate on the Right to Be Forgotten involves navigating a delicate balance between individual privacy and societal interests, and opinions can be shaped by a range of ethical, cultural, and legal considerations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89533268-bbd0-456c-be7b-4e12d49c6e28",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25d5765",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f1da21",
   "metadata": {},
   "source": [
    "# Comment on Two Articles\n",
    "\n",
    "Comment on two of the articles that were discussed in this lesson and that you have read carefully.   For each, say in 1-2 paragraphs:\n",
    "\n",
    "- Which article you are referring to\n",
    "\n",
    "- What you found most compelling or striking about the article\n",
    "\n",
    "- What you feel is the key ethical issue illustrated by the article\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33352115",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d8e183-8dc4-4771-ae96-ed57c4659b5f",
   "metadata": {},
   "source": [
    "https://www.nytimes.com/2020/12/03/technology/google-researcher-timnit-gebru.html?campaign_id=2&emc=edit_th_20201204&instance_id=24702&nl=todaysheadlines&regi_id=17754076&segment_id=46051&user_id=16ecf3987642ea9959b524420f653d1e\n",
    "\n",
    "nytimes.com\n",
    "Google Researcher Timnit Gebru Says She Was Fired For Paper on AI Bias\n",
    "Cade Metz, Daisuke Wakabayashi\n",
    "9–11 minutes\n",
    "\n",
    "\n",
    "\n",
    "Google Researcher Says She Was Fired Over Paper Highlighting Bias in A.I.The article portrays the compelling narrative of Timnit Gebru's departure from Google, shedding light on the intricate challenges faced by those advocating for diversity and addressing bias in artificial intelligence (AI). Gebru's dismissal following her critique of Google's handling of minority hiring and AI bias underscores the tension between corporate interests and ethical considerations within the tech industry. This situation epitomizes the clash between profit-driven motives and the pursuit of responsible AI development, raising pertinent questions about transparency, accountability, and the prioritization of societal welfare over corporate gains. Moreover, Gebru's experience underscores broader issues of racial inequality and discrimination within Silicon Valley, emphasizing the imperative for greater diversity, transparency, and ethical accountability in AI development and deployment processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af3f7b5-2da0-4a6c-9279-d8a9ec8f4d26",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4857ad38-8bbf-4ecb-b41f-af02a91c336e",
   "metadata": {},
   "source": [
    "https://www.technologyreview.com/2020/12/04/1013294/google-ai-ethics-research-paper-forced-out-timnit-gebru/\n",
    "\n",
    "\n",
    "technologyreview.com\n",
    "We read the paper that forced Timnit Gebru out of Google. Here’s what it says.\n",
    "Karen Hao\n",
    "10–13 minutes\n",
    "\n",
    "On the evening of Wednesday, December 2, Timnit Gebru, the co-lead of Google’s ethical AI team, announced via Twitter that the company had forced her out. \n",
    "\n",
    "The article presents the compelling account of Timnit Gebru's departure from Google, highlighting the contentious circumstances surrounding her exit and the implications for AI ethics research. Gebru's influential work on bias in facial recognition and her efforts to champion diversity within the tech industry underscore the significance of her departure. The conflict appears to center around a research paper coauthored by Gebru that critiques the environmental, social, and ethical implications of large language models in AI development. The paper raises pertinent concerns regarding the environmental and financial costs of training such models, the perpetuation of biases in training data, the misdirection of research efforts, and the potential for these models to generate misleading or harmful information. The response from Google's AI head, Jeff Dean, and the subsequent fallout within the AI ethics community highlight broader concerns about corporate influence on research integrity and the need to safeguard academic freedom and ethical inquiry in the pursuit of AI innovation. Gebru's departure and the controversy surrounding her research paper serve as a stark reminder of the ethical complexities inherent in AI development and the importance of fostering a culture of transparency, accountability, and diversity within the tech industry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ea6ec8-5c34-40c0-afef-454202fd96c4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d66e31",
   "metadata": {},
   "source": [
    "# Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b563930f",
   "metadata": {},
   "source": [
    "# Facial Recognition\n",
    "\n",
    "Provide an answer of 1-2 paragraphs (or more if you want and feel that is needed) to each of these questions:\n",
    "\n",
    "- Given the current imperfect but improving state of facial recognition software, where (if at all) do you think it should and/or should not be used?   \n",
    "\n",
    "- Suppose facial recognition improved to a point where identifications are nearly flawless.   Then, where (if at all) do you think it should and/or should not be used?  \n",
    "\n",
    "In answering each question, please consider at least two potential uses of facial recognition, such as police identification of people sought for questioning, government identification of suspected potential terrorists, or perhaps more benign applications like professors taking attendance in large lectures\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb5b48c",
   "metadata": {},
   "source": [
    "**Given the current imperfect but improving state of facial recognition software, where (if at all) do you think it should and/or should not be used?**\n",
    "\n",
    "Facial recognition technology, in its current state, has demonstrated imperfections, including issues related to accuracy, bias, and privacy concerns. Considering these limitations, it should be cautiously employed in areas where the consequences of misidentification are less severe. For instance, benign applications like professors using facial recognition for attendance in large lectures could be suitable, as the impact of an occasional error is minimal. However, in high-stakes scenarios such as law enforcement using facial recognition for criminal investigations, its current limitations may pose significant risks, leading to potential false arrests or other serious consequences. Until the technology is more reliable and bias-free, its use in critical applications should be approached with caution.\n",
    "\n",
    "**Suppose facial recognition improved to a point where identifications are nearly flawless. Then, where (if at all) do you think it should and/or should not be used?**\n",
    "\n",
    "If facial recognition were to reach a point of near-perfection, it could be more confidently employed in various applications. In law enforcement, for instance, accurate facial recognition could be valuable for identifying and apprehending individuals sought for questioning, provided that strict safeguards are in place to prevent misuse and protect individual rights. Government identification of suspected potential terrorists could also benefit from highly accurate facial recognition, given appropriate oversight and adherence to legal and ethical standards. However, even with improved accuracy, concerns about privacy and the potential for abuse would still need careful consideration. In more benign applications, such as attendance tracking or access control, near-flawless facial recognition could be more widely adopted, provided strict privacy measures are maintained to prevent unauthorized use or tracking. Overall, as the technology improves, its ethical and responsible deployment becomes increasingly crucial to prevent misuse and protect individual rights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef71d5a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e14002",
   "metadata": {},
   "source": [
    "# Week 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc750db2",
   "metadata": {},
   "source": [
    "# Gene Editing and Neurological Interventions\n",
    "\n",
    "Provide an answer of 1-2 paragraphs to each of these questions:\n",
    "\n",
    "- Under what conditions do you think it is ok to change the capabilities of a human being via gene editing and/or neurological intervention?   E.g. diseases that significantly impact the length and quality of life?   Conditions that make certain people less physically or mentally capable than others?   Creating AI-enhanced superhumans, if we ever can?    What is the basis for your opinion (can be anything that is meaningful to you)?\n",
    "\n",
    "- For these interventions to progress and succeed, significant involvement from data scientists will be required.   What do you think the stance of the data science community should be with respect to doing such work?   This could include whether or not, or under what conditions, you think data scientists should work on these problems; what obligations you feel the data science community has to be part of the broader ethical discussion on these topics; or other aspects that you feel are important.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41012ec",
   "metadata": {},
   "source": [
    "**Under what conditions do you think it is okay to change the capabilities of a human being via gene editing and/or neurological intervention?**\n",
    "\n",
    "The ethical considerations surrounding gene editing and neurological interventions are complex and often subjective. Generally, it could be ethically justifiable to use these technologies under conditions where they address severe genetic diseases that significantly impact the length and quality of life. Additionally, interventions might be considered when aiming to correct conditions that create substantial disparities in physical or mental capabilities, as long as the goal is to enhance overall well-being and equity. However, caution is needed to prevent unintended consequences, and careful consideration of ethical, cultural, and societal implications is crucial. The basis for this opinion lies in balancing the potential for medical advancements with a commitment to respecting individual autonomy, preventing discrimination, and promoting social justice.\n",
    "\n",
    "**For these interventions to progress and succeed, significant involvement from data scientists will be required. What do you think the stance of the data science community should be with respect to doing such work?**\n",
    "\n",
    "The data science community should play a pivotal role in the ethical and responsible development of gene editing and neurological interventions. Data scientists should actively engage in discussions about the ethical implications of their work, contributing to a broader understanding of the potential risks and benefits. They should adhere to transparent and inclusive practices, incorporating diverse perspectives to address societal concerns. There should be a commitment to avoiding biases in algorithms and ensuring that the benefits of these technologies are accessible to all, without exacerbating existing social inequalities. Additionally, data scientists should advocate for ethical guidelines and regulations to govern the use of their work in gene editing and neurological interventions, fostering a responsible and accountable approach to the intersection of data science and human enhancement technologies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbca0477",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb97174",
   "metadata": {},
   "source": [
    "# The Future of Work\n",
    "\n",
    "Provide an answer of 1-2 paragraphs to at least two of the following questions:\n",
    "\n",
    "- Do the people or organizations that are behind the changes which cause jobs to disappear have an obligation to help the people who lose jobs?\n",
    "\n",
    "- Who has an obligation to help people retrain – governments, companies, …?\n",
    "\n",
    "- In times of rapid job dislocations, do governments or past employers have an obligation to provide greater safety nets for people who lose jobs?\n",
    "\n",
    "- Will we enter a world where there is sufficient wealth for all but not sufficient work for all – and if so, should society provide a guaranteed basic income for all?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96315f8a",
   "metadata": {},
   "source": [
    "**Do the people or organizations that are behind the changes which cause jobs to disappear have an obligation to help the people who lose jobs?**\n",
    "\n",
    "Yes, there is a compelling ethical argument that those responsible for technological or organizational changes resulting in job displacement have an obligation to support affected individuals. Whether it's due to automation, technological advancements, or shifts in industry, these changes often leave workers without the skills required for new job opportunities. The responsible entities, whether companies or organizations, should contribute to retraining programs, provide financial assistance during transitions, and support initiatives that facilitate the adaptation of the workforce. A collaborative effort between the private and public sectors can help mitigate the negative consequences of job displacement and foster a more inclusive and sustainable job market.\n",
    "\n",
    "\n",
    "**Who has an obligation to help people retrain – governments, companies, …?**\n",
    "\n",
    "The responsibility for helping people retrain is a shared obligation that involves both governments and companies. Governments play a crucial role in creating and implementing policies that facilitate access to education and training programs, ensuring they are affordable and widely available. They can also incentivize businesses to invest in employee training through tax incentives or other means. On the other hand, companies have a direct interest in maintaining a skilled and adaptable workforce. They should invest in training and upskilling initiatives, offering resources and support for employees to acquire new skills. A collaborative effort between governments and companies is essential to create a dynamic and responsive workforce capable of navigating evolving job markets.\n",
    "\n",
    "**In times of rapid job dislocations, do governments or past employers have an obligation to provide greater safety nets for people who lose jobs?**\n",
    "\n",
    "In times of rapid job dislocations, both governments and past employers have a shared responsibility to provide greater safety nets for individuals who lose their jobs. Governments should establish and maintain robust social safety nets, including unemployment benefits, healthcare coverage, and retraining programs, to support workers during periods of transition. This ensures that individuals have the means to meet their basic needs and can actively pursue new opportunities without facing severe financial hardships. Past employers also bear responsibility, particularly if job displacement results from technological advancements or organizational changes. They should consider providing severance packages, outplacement services, and support for retraining to help former employees navigate the challenges of finding new employment. This collaborative approach can contribute to a more resilient and compassionate response to the societal impacts of rapid job dislocations.\n",
    "\n",
    "**Will we enter a world where there is sufficient wealth for all but not sufficient work for all – and if so, should society provide a guaranteed basic income for all?**\n",
    "\n",
    "As technology continues to advance and certain jobs become automated, there is a possibility of entering a world where there is sufficient wealth but not enough traditional employment opportunities for everyone. In such a scenario, the concept of a guaranteed basic income (UBI) becomes increasingly relevant. UBI could provide financial security for individuals, ensuring a baseline standard of living regardless of employment status. Implementing a UBI would require careful consideration of economic feasibility, societal values, and potential consequences. It could potentially empower individuals to pursue education, entrepreneurship, or community service, fostering a more dynamic and adaptable workforce in the face of evolving economic landscapes. However, discussions around UBI should involve a comprehensive examination of its impact on societal structures, economic incentives, and the overall well-being of the population."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f224f4-483d-42d4-9133-8ccc9c7346c0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81310086-06d1-4eb7-8a8f-5148bf3cb12e",
   "metadata": {},
   "source": [
    "# Week 3 articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82e864d-1bd8-441b-913e-6d6097182f1a",
   "metadata": {},
   "source": [
    "https://www.nytimes.com/2020/12/03/technology/google-researcher-timnit-gebru.html?campaign_id=2&emc=edit_th_20201204&instance_id=24702&nl=todaysheadlines&regi_id=17754076&segment_id=46051&user_id=16ecf3987642ea9959b524420f653d1e\n",
    "\n",
    "nytimes.com\n",
    "Google Researcher Timnit Gebru Says She Was Fired For Paper on AI Bias\n",
    "Cade Metz, Daisuke Wakabayashi\n",
    "9–11 minutes\n",
    "\n",
    "\n",
    "\n",
    "Google Researcher Says She Was Fired Over Paper Highlighting Bias in A.I.\n",
    "\n",
    "Timnit Gebru, one of the few Black women in her field, had voiced exasperation over the company’s response to efforts to increase minority hiring.\n",
    "Timnit Gebru, a respected researcher at Google, questioned biases built into artificial intelligence systems.Credit...Cody O'Loughlin for The New York Times\n",
    "Timnit Gebru, a respected researcher at Google, questioned biases built into artificial intelligence systems.\n",
    "\n",
    "A well-respected Google researcher said she was fired by the company after criticizing its approach to minority hiring and the biases built into today’s artificial intelligence systems.\n",
    "\n",
    "Timnit Gebru, who was a co-leader of Google’s Ethical A.I. team, said in a tweet on Wednesday evening that she was fired because of an email she had sent a day earlier to a group that included company employees.\n",
    "\n",
    "In the email, reviewed by The New York Times, she expressed exasperation over Google’s response to efforts by her and other employees to increase minority hiring and draw attention to bias in artificial intelligence.\n",
    "\n",
    "“Your life starts getting worse when you start advocating for underrepresented people. You start making the other leaders upset,” the email read. “There is no way more documents or more conversations will achieve anything.”\n",
    "\n",
    "Her departure from Google highlights growing tension between Google’s outspoken work force and its buttoned-up senior management, while raising concerns over the company’s efforts to build fair and reliable technology. It may also have a chilling effect on both Black tech workers and researchers who have left academia in recent years for high-paying jobs in Silicon Valley.\n",
    "\n",
    "“Her firing only indicates that scientists, activists and scholars who want to work in this field — and are Black women — are not welcome in Silicon Valley,” said Mutale Nkonde, a fellow with the Stanford Digital Civil Society Lab. “It is very disappointing.”\n",
    "\n",
    "A Google spokesman declined to comment. In an email sent to Google employees, Jeff Dean, who oversees Google’s A.I. work, including that of Dr. Gebru and her team, called her departure “a difficult moment, especially given the important research topics she was involved in, and how deeply we care about responsible A.I. research as an org and as a company.”\n",
    "\n",
    "After years of an anything-goes environment where employees engaged in freewheeling discussions in companywide meetings and online message boards, Google has started to crack down on workplace discourse. Many Google employees have bristled at the new restrictions and have argued that the company has broken from a tradition of transparency and free debate.\n",
    "\n",
    "On Wednesday, the National Labor Relations Board said Google had most likely violated labor law when it fired two employees who were involved in labor organizing. The federal agency said Google illegally surveilled the employees before firing them.\n",
    "\n",
    "Google’s battles with its workers, who have spoken out in recent years about the company’s handling of sexual harassment and its work with the Defense Department and federal border agencies, have diminished its reputation as a utopia for tech workers with generous salaries, perks and workplace freedom.\n",
    "\n",
    "Like other technology companies, Google has also faced criticism for not doing enough to resolve the lack of women and racial minorities among its ranks.\n",
    "\n",
    "The problems of racial inequality, especially the mistreatment of Black employees at technology companies, has plagued Silicon Valley for years. Coinbase, the most valuable cryptocurrency start-up, has experienced an exodus of Black employees in the last two years over what the workers said was racist and discriminatory treatment.\n",
    "\n",
    "Researchers worry that the people who are building artificial intelligence systems may be building their own biases into the technology. Over the past several years, several public experiments have shown that the systems often interact differently with people of color — perhaps because they are underrepresented among the developers who create those systems.\n",
    "\n",
    "Dr. Gebru, 37, was born and raised in Ethiopia. In 2018, while a researcher at Stanford University, she helped write a paper that is widely seen as a turning point in efforts to pinpoint and remove bias in artificial intelligence. She joined Google later that year, and helped build the Ethical A.I. team.\n",
    "\n",
    "After hiring researchers like Dr. Gebru, Google has painted itself as a company dedicated to “ethical” A.I. But it is often reluctant to publicly acknowledge flaws in its own systems.\n",
    "\n",
    "In an interview with The Times, Dr. Gebru said her exasperation stemmed from the company’s treatment of a research paper she had written with six other researchers, four of them at Google. The paper, also reviewed by The Times, pinpointed flaws in a new breed of language technology, including a system built by Google that underpins the company’s search engine.\n",
    "\n",
    "These systems learn the vagaries of language by analyzing enormous amounts of text, including thousands of books, Wikipedia entries and other online documents. Because this text includes biased and sometimes hateful language, the technology may end up generating biased and hateful language.\n",
    "\n",
    "After she and the other researchers submitted the paper to an academic conference, Dr. Gebru said, a Google manager demanded that she either retract the paper from the conference or remove her name and the names of the other Google employees. She refused to do so without further discussion and, in the email sent Tuesday evening, said she would resign after an appropriate amount of time if the company could not explain why it wanted her to retract the paper and answer other concerns.\n",
    "\n",
    "The company responded to her email, she said, by saying it could not meet her demands and that her resignation was accepted immediately. Her access to company email and other services was immediately revoked.\n",
    "\n",
    "In his note to employees, Mr. Dean said Google respected “her decision to resign.” Mr. Dean also said that the paper did not acknowledge recent research showing ways of mitigating bias in such systems.\n",
    "\n",
    "“It was dehumanizing,” Dr. Gebru said. “They may have reasons for shutting down our research. But what is most upsetting is that they refuse to have a discussion about why.”\n",
    "\n",
    "Dr. Gebru’s departure from Google comes at a time when A.I. technology is playing a bigger role in nearly every facet of Google’s business. The company has hitched its future to artificial intelligence — whether with its voice-enabled digital assistant or its automated placement of advertising for marketers — as the breakthrough technology to make the next generation of services and devices smarter and more capable.\n",
    "\n",
    "Sundar Pichai, chief executive of Alphabet, Google’s parent company, has compared the advent of artificial intelligence to that of electricity or fire, and has said that it is essential to the future of the company and computing. Earlier this year, Mr. Pichai called for greater regulation and responsible handling of artificial intelligence, arguing that society needs to balance potential harms with new opportunities.\n",
    "\n",
    "Google has repeatedly committed to eliminating bias in its systems. The trouble, Dr. Gebru said, is that most of the people making the ultimate decisions are men. “They are not only failing to prioritize hiring more people from minority communities, they are quashing their voices,” she said.\n",
    "\n",
    "Julien Cornebise, an honorary associate professor at University College London and a former researcher with DeepMind, a prominent A.I. lab owned by the same parent company as Google’s, was among many artificial intelligence researchers who said Dr. Gebru’s departure reflected a larger problem in the industry.\n",
    "\n",
    "“This shows how some large tech companies only support ethics and fairness and other A.I.-for-social-good causes as long as their positive P.R. impact outweighs the extra scrutiny they bring,” he said. “Timnit is a brilliant researcher. We need more like her in our field.”\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c69343-80db-4217-9b36-04c0eda6d7a3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bc499e-371b-425b-bad2-45fdac4d974b",
   "metadata": {},
   "source": [
    "https://www.technologyreview.com/2020/12/04/1013294/google-ai-ethics-research-paper-forced-out-timnit-gebru/\n",
    "\n",
    "\n",
    "technologyreview.com\n",
    "We read the paper that forced Timnit Gebru out of Google. Here’s what it says.\n",
    "Karen Hao\n",
    "10–13 minutes\n",
    "\n",
    "On the evening of Wednesday, December 2, Timnit Gebru, the co-lead of Google’s ethical AI team, announced via Twitter that the company had forced her out. \n",
    "\n",
    "Gebru, a widely respected leader in AI ethics research, is known for coauthoring a groundbreaking paper that showed facial recognition to be less accurate at identifying women and people of color, which means its use can end up discriminating against them. She also cofounded the Black in AI affinity group, and champions diversity in the tech industry. The team she helped build at Google is one of the most diverse in AI and includes many leading experts in their own right. Peers in the field envied it for producing critical work that often challenged mainstream AI practices.\n",
    "\n",
    "A series of tweets, leaked emails, and media articles showed that Gebru’s exit was the culmination of a conflict over another paper she coauthored. Jeff Dean, the head of Google AI, told colleagues in an internal email (which he has since put online) that the paper “didn’t meet our bar for publication” and that Gebru had said she would resign unless Google met a number of conditions, which it was unwilling to meet. Gebru tweeted that she had asked to negotiate “a last date” for her employment after she got back from vacation. She was cut off from her corporate email account before her return.\n",
    "\n",
    "Online, many other leaders in the field of AI ethics are arguing that the company pushed her out because of the inconvenient truths that she was uncovering about a core line of its research—and perhaps its bottom line. More than 1,400 Google staff members and 1,900 other supporters have also signed a letter of protest.\n",
    "\n",
    "Many details of the exact sequence of events that led up to Gebru’s departure are not yet clear; both she and Google have declined to comment beyond their posts on social media. But MIT Technology Review obtained a copy of the research paper from  one of the coauthors, Emily M. Bender, a professor of computational linguistics at the University of Washington. Though Bender asked us not to publish the paper itself because the authors didn’t want such an early draft circulating online, it gives some insight into the questions Gebru and her colleagues were raising about AI that might be causing Google concern.\n",
    "\n",
    "“On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?” lays out the risks of large language models—AIs trained on staggering amounts of text data. These have grown increasingly popular—and increasingly large—in the last three years. They are now extraordinarily good, under the right conditions, at producing what looks like convincing, meaningful new text—and sometimes at estimating meaning from language. But, says the introduction to the paper, “we ask whether enough thought has been put into the potential risks associated with developing them and strategies to mitigate these risks.”\n",
    "The paper\n",
    "\n",
    "The paper, which builds on the work of other researchers, presents the history of natural-language processing, an overview of four main risks of large language models, and suggestions for further research. Since the conflict with Google seems to be over the risks, we’ve focused on summarizing those here. \n",
    "Environmental and financial costs\n",
    "\n",
    "Training large AI models consumes a lot of computer processing power, and hence a lot of electricity. Gebru and her coauthors refer to a 2019 paper from Emma Strubell and her collaborators on the carbon emissions and financial costs of large language models. It found that their energy consumption and carbon footprint have been exploding since 2017, as models have been fed more and more data.\n",
    "\n",
    "Strubell’s study found that training one language model with a particular type of “neural architecture search” (NAS) method would have produced the equivalent of 626,155 pounds (284 metric tons) of carbon dioxide—about the lifetime output of five average American cars. Training a version of Google’s language model, BERT, which underpins the company’s search engine, produced 1,438 pounds of CO2 equivalent in Strubell’s estimate—nearly the same as a round-trip flight between New York City and San Francisco. These numbers should be viewed as minimums, the cost of training a model one time through. In practice, models are trained and retrained many times over during research and development.\n",
    "\n",
    "Gebru’s draft paper points out that the sheer resources required to build and sustain such large AI models means they tend to benefit wealthy organizations, while climate change hits marginalized communities hardest. “It is past time for researchers to prioritize energy efficiency and cost to reduce negative environmental impact and inequitable access to resources,” they write.\n",
    "Massive data, inscrutable models\n",
    "\n",
    "Large language models are also trained on exponentially increasing amounts of text. This means researchers have sought to collect all the data they can from the internet, so there's a risk that racist, sexist, and otherwise abusive language ends up in the training data.\n",
    "\n",
    "An AI model taught to view racist language as normal is obviously bad. The researchers, though, point out a couple of more subtle problems. One is that shifts in language play an important role in social change; the MeToo and Black Lives Matter movements, for example, have tried to establish a new anti-sexist and anti-racist vocabulary. An AI model trained on vast swaths of the internet won’t be attuned to the nuances of this vocabulary and won’t produce or interpret language in line with these new cultural norms.\n",
    "\n",
    "It will also fail to capture the language and the norms of countries and peoples that have less access to the internet and thus a smaller linguistic footprint online. The result is that AI-generated language will be homogenized, reflecting the practices of the richest countries and communities.\n",
    "\n",
    "Moreover, because the training data sets are so large, it’s hard to audit them to check for these embedded biases. “A methodology that relies on datasets too large to document is therefore inherently risky,” the researchers conclude. “While documentation allows for potential accountability, [...] undocumented training data perpetuates harm without recourse.”\n",
    "Research opportunity costs\n",
    "\n",
    "The researchers summarize the third challenge as the risk of “misdirected research effort.” Though most AI researchers acknowledge that large language models don’t actually understand language and are merely excellent at manipulating it, Big Tech can make money from models that manipulate language more accurately, so it keeps investing in them. “This research effort brings with it an opportunity cost,” Gebru and her colleagues write. Not as much effort goes into working on AI models that might achieve understanding, or that achieve good results with smaller, more carefully curated data sets (and thus also use less energy).\n",
    "Illusions of meaning\n",
    "\n",
    "The final problem with large language models, the researchers say, is that because they’re so good at mimicking real human language, it’s easy to use them to fool people. There have been a few high-profile cases, such as the college student who churned out AI-generated self-help and productivity advice on a blog, which went viral.\n",
    "\n",
    "The dangers are obvious: AI models could be used to generate misinformation about an election or the covid-19 pandemic, for instance. They can also go wrong inadvertently when used for machine translation. The researchers bring up an example: In 2017, Facebook mistranslated a Palestinian man’s post, which said “good morning” in Arabic, as “attack them” in Hebrew, leading to his arrest.\n",
    "Why it matters\n",
    "\n",
    "Gebru and Bender’s paper has six coauthors, four of whom are Google researchers. Bender asked to avoid disclosing their names for fear of repercussions. (Bender, by contrast, is a tenured professor: “I think this is underscoring the value of academic freedom,” she says.)\n",
    "\n",
    "The paper’s goal, Bender says, was to take stock of the landscape of current research in natural-language processing. “We are working at a scale where the people building the things can’t actually get their arms around the data,” she said. “And because the upsides are so obvious, it’s particularly important to step back and ask ourselves, what are the possible downsides? … How do we get the benefits of this while mitigating the risk?”\n",
    "\n",
    "In his internal email, Dean, the Google AI head, said one reason the paper “didn’t meet our bar” was that it “ignored too much relevant research.” Specifically, he said it didn’t mention more recent work on how to make large language models more energy efficient and mitigate problems of bias. \n",
    "\n",
    "However, the six collaborators drew on a wide breadth of scholarship. The paper’s citation list, with 128 references, is notably long. “It’s the sort of work that no individual or even pair of authors can pull off,” Bender said. “It really required this collaboration.” \n",
    "\n",
    "The version of the paper we saw does also nod to several research efforts on reducing the size and computational costs of large language models, and on measuring the embedded bias of models. It argues, however, that these efforts have not been enough. “I’m very open to seeing what other references we ought to be including,” Bender said.\n",
    "\n",
    "Nicolas Le Roux, a Google AI researcher in the Montreal office, later noted on Twitter that the reasoning in Dean’s email was unusual. “My submissions were always checked for disclosure of sensitive material, never for the quality of the literature review,” he said.\n",
    "\n",
    "Dean’s email also says that Gebru and her colleagues gave Google AI only a day for an internal review of the paper before they submitted it to a conference for publication. He wrote that “our aim is to rival peer-reviewed journals in terms of the rigor and thoughtfulness in how we review research before publication.”\n",
    "\n",
    "Bender noted that even so, the conference would still put the paper through a substantial review process: “Scholarship is always a conversation and always a work in progress,” she said. \n",
    "\n",
    "Others, including William Fitzgerald, a former Google PR manager, have further cast doubt on Dean’s claim.\n",
    "\n",
    "Google pioneered much of the foundational research that has since led to the recent explosion in large language models. Google AI was the first to invent the Transformer language model in 2017 that serves as the basis for the company’s later model BERT, and OpenAI’s GPT-2 and GPT-3. BERT, as noted above, now also powers Google search, the company’s cash cow.\n",
    "\n",
    "Bender worries that Google’s actions could create “a chilling effect” on future AI ethics research. Many of the top experts in AI ethics work at large tech companies because that is where the money is. “That has been beneficial in many ways,” she says. “But we end up with an ecosystem that maybe has incentives that are not the very best ones for the progress of science for the world.”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878ed50c-10ec-4d74-b44e-3e4e119fbba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
