We'll come back to this
lecture on sorting, where we are going to introduce the sorting problem and talk about our first
sorting algorithm, which is going to
be insertion sort. What is sorting? Sorting is a very, very common operation. I'm sure you've already
done this once, suppose you have a bunch
of sheets of paper, you have a bunch of
exams from students. Let's say from students
like Jack Smith, Jane, I don't know something like Jane A. I'm not going to
give names to everyone, John X, Jake B, suppose we have a bunch
of students and we would like to arrange them in alphabetical order
by their last names. What we really need
is to first put Jane, her last name starts with an a, followed by Jake, whose last name starts with
a B, followed by Jack. I'm not very, very inventive in my names here, followed by John. Maybe we could add
a Bob in there. I'm just being facetious here, but the point is, we would like to sort. What does sorting mean? Sorting is a process of ordering. You're given a bunch of inputs and you are
given an ordering, so you're given a bunch of
inputs and in computers, our inputs are
going to be arrays. Let's take them to be arrays
of numbers because numbers are easy to write on the board. I don't have to invent names, I just have to invent numbers. We are given a bunch of numbers and I would like to sort
them according to an order. According to an order
in mathematics, we would call this a total order. According to a total order. In this case, the order is just a less than or equal to relation. I would like to sort them in ascending order according to less than or equal to relation. The smallest number is four, followed by 12, followed
by 15, followed by 18, 19, 20, 21, 23, and 45, I believe that's it. What are the key
requirements for sorting? There are two requirements. Number one is that
the output must have exactly same
elements as inputs. You cannot have a new element in the output array that
wasn't in the input array. Every element in the input array is somewhere in the output array. You are only allowed
to reorder things, you're not allowed to delete or insert new elements
and likewise, you cannot have an element here that wasn't in the input so every element in the output
is already in the input. The input and output, the only thing you're allowed
to do is permute them, only thing you're allowed
to do is permute the array. You're not allowed
to add new elements, you are not allowed
to delete elements. That's the first requirement. The second requirement
is output must be sorted according to
the provided order. If less than equal
to is the order, then the output must be
sorted in that order, so four must be less
than equal to 12, must be less than equal to 15, must be less than equal to 18. This is what it means to
sort in ascending order. If you wanted to sort
in descending order, instead of giving less than equal to as the ordering relation, I would have given you
greater than or equal to. So far I'm just going to talk about
ascending order sorts. But descending
order sort is very, very similar wherever you
see less than equal to, make it greater than equal to. Formally, what is
the sorting problem? For the sorting problem, the input is an array, A1, A2 and this is going to be another annoying convention where we deviate from
Python in this class. Our arrays are going to
start from one and end at n. Whenever you translate code from pseudocode
that we will write, I will show you in a second what it means to write pseudo-code, whenever you translate that, you better be careful that
arrays move from one to n. I'm going to write this as S of one or a underscore one, a of n. The output, could be at same array, it
could be a fresh array, but now the elements are sorted. Let me call them a hat of one, a hat of two, a hat of n. What does it mean? For the set, or more
importantly, the multiset, a hat of one to a
hat of n must be the same as the set of elements
a of one to a of n. You cannot have new
elements come in, it must be the same
set of elements. The set of elements
must be preserved. The second thing is, and this is the same
as requirement one, a hat of one must be less
than equal to a hat of two, must be less than equal
to all the way less than equal to a hat of N. This is the second
requirement here. You want to sort an array. How do you sort an
array of N elements? Now sorting is really, really common. Where do you see sorting? Anytime you load an
Excel spreadsheet, you can sort each column
according to some criteria. For example, if it's a
column of strings like so, you could sort them
according to the last name. Then instead of the order being the less than
equal to among numbers, the order is the
alphabetical order or the lexicographic order. Here the sorting order is
the lexicographic order. I'm writing it in short for lex, which simply means it's
the dictionary order. Here the sorting is the increasing order given by the less than or
equal to relation. Not only can you sort numbers, you can sort strings, you can sort all
kinds of objects. For example, you could
overload in many languages, you could do operator overload, and you could overload
the less than or equal to or the compile operator. Once you overload this
compile operator, you can call a generic
sorting function to sort an array of such objects. You could do this in Python, you could do this in C++, you could do this in Java, all of them have generic
sorting functions that can not only sort numbers, they can sort strings, they can sort classes,
they can sort whatever, they can sort dictionaries, but you need to specify
an ordering relation. Once you do that, it just becomes a generic sorting algorithm that applies on that
ordering relation. But for this class we are
only going to study arrays of numbers and we will
use less than equal to, so we will do ascending
sorting. All right. What about sorting algorithms? Sorting turns out to be
a well-studied problem. Number 1, because it's so useful we would
really like to sort. Often, we don't want
to sort small arrays, we would like to sort
large collections of data. One of the things that
doesn't get mentioned in big data is the need
to organize big data, and for that you need efficient
algorithms like sorting. We need to sort large
numbers of arrays. With this in mind,
there are plenty of algorithms for sorting. The most common algorithms for sorting and we
will study them. Insertion sort, which we
are going to study now. There's something called
bubble sort which I will skip but it's there in your
book, you can look at it. There is something
called merge sort, heapsort, quicksort. Then there are combination
sorting algorithms that take different nice characteristics of these algorithms and do. There's something called Timsort which is considered to be the state of the art
in sorting algorithms. It's implemented by many
languages including Python and these include many of the best characteristics of
these sorting algorithms. The one that's most commonly used these days is quicksort, insertion sort for small arrays. Typically a combination of
quicksort and insertion sort, we will study this in this
class in a lot of detail. Typically a combination
of these two turns out to be the fastest
sorting algorithm. You can do much more
sophisticated ones but if you want to get a fast
sorting algorithm done, you could do it through
a combination of quicksort and insertion sort. But let's start by studying a simple sorting algorithm
which is the insertion sort. What is the insertion
sorting algorithm? This we will study
in the next lecture. We will start in
the next lecture, we are going to study this
first sorting algorithm which is called the
insertion sort. See you then. Bye.
Oh, welcome back. In this lecture, we
are going to study the insertion sort which is going to be the very
first sorting algorithm we'll study in this class. First, let me give you the
idea of insertion sort. In the previous lecture, we looked at what sorting means. Let's now look at the
idea of insertion sort. The idea here is, suppose we
are given an input array, let's say 2,7,4,1,3,6,5, and 0. Let's say this is
our initial array. What we're going to do is
we're going to go through each of the elements of this
array from beginning to end. What we are going to do in insertion sort is roughly we are going to do the following. We're going to maintain
two parts of the array. The first part of the array, which I'll draw in pink
is going to be assorted. It's going to look something
like this 1, 2, 4, 7. This is going to be the
part that's sorted. Then comes the part
that's unsorted, let's say 3, 6, 5, 0. Here, for example,
I would have sorted the first four elements and the rest of the
array is untouched. Then each step of
insertion sort is going to grow the pink region by one. Extend the pink region
by one more element. How does that work?
That works like this. Suppose the pink region now has the first four elements 2, 7, 4, 1 has been sorted. The first four elements of
the array have been sorted. This is somewhere in the middle. The pointer comes to three. What happens is this is
called the insert step. To grow the pink region
by one more element, I'm going to need to insert
three into this array. How does three get inserted? First I look for the
right place for three. I'll start scanning
from the left, and I will see three
needs to come here. But because this is an array, if I'm going to
insert three here, then four and seven
need scooch over. They need to move over. Four needs to move one
place to the left, seven needs to move one
place to the right. Because we are mirrored over, to you it will look like
one place to the left, to me it looks to the
right to you to the left. How does this happen? At the Insert step,
there is two parts first find where three needs to go. Next, perform the insertion and move everything over. Sorry about the spelling there. How does that look? In this case, that looks like this, you would end up
with one and two, and then the new element
three which I'll write in green gets inserted, everything else moves over. The sorted portion includes the first five
elements of the array, with the remaining ones being, I'll write them in blue 6, 5,0. The pointer moves to six, so six gets inserted next. What happens where
the six get inserted, again the same procedure
A and B gets run all over again and now
where the six get inserted. Six gets inserted right here
between four and seven. That's where it's going to get inserted. How do I know that? I'm going to scan
from the beginning of the array until I find an
element that's larger than six, and then I know six
needs to get inserted there. How does that work? Then once I do that, I'm going to insert six there, now I get a new array 1, 2, 3, 4, 6, 7. Let me follow my convention and put a green thing under six, that's the new thing
that gets inserted, followed by five and zero. Five and zero I should
have written them in blue but I'm simply going to put a blue box
around them so that you know. This is the basic idea. What is insertion sort? Insertion sort goes like this. Initially the entire array is in blue which means
nothing is sorted, and then I start with
the very first element. When I start with the very
first element which is two, it's just one element, so it's already sorted. One element, there's nothing to sort, it's already sorted, and then I have 7, 4, 1, 3, 6, 5, 0. Now seven gets inserted
and when you scan, you'll find that
seven is already in the right place and therefore, you trivially insert it by
just keeping it where it is. Now you get two, seven, and then the remaining
ones are four, one, three, six, five, zero, and then the pointer
goes and points to four. Then the third step,
four gets inserted, and four should get actually inserted somewhere
between two and seven, therefore it becomes two, four, seven, and then one, three, six, five, zero. As you can see, the blue
part remains untouched. The pink part is always sorted, so the first three
elements are now sorted. Now the 4th element comes in, it has to go to the
very beginning right here now you get one, two, four, seven and then the
remaining are just the same, three, six, five, zero. All right? And the cursor
goes to the three, and you can see some of
the steps here following. Eventually what happens
is when this algorithm finishes you get zero, one, two, three, four,
five, six, seven. The entire array gets sorted. This is the basic idea
behind insertion sort. In the next lecture, I'm going to write
down the pseudocode for insertion sort, and we'll actually first start by writing the pseudocode
for the insert procedure, to insert a new element
into the array, and once we are done with
the insert procedure, we are going to
complete the insertion sort procedure as a pseudocode, and the lecture after that, we are going to do the
complexity analysis and measure the running time or study the running time
of insertion sort. See you then bye. I' m back, and now we're going to study the insert method
for insertion sort. We are going to study
the insert method, in the insert method, we are going to take an
array a and then index j. We are going to assume that
a has a of one to a of n and somewhere in
the middle is a of j. j doesn't have
to be n minus one, so it's somewhere in the middle, let me write some dots here. We have a of one to a of n. Now, I would like to insert a of j, and we will assume this is very important
that this is the part from a of one to aj
minus one is sorted. It's sorted so I'm going
to put a box next to it, a pink box, and a of j is
what we are trying to insert. This is to be inserted. The way I explained so far
is you are going to look from your right to left. Remember I am inverted
so we're going to look from one end all the way, find the insert position, the way we set it so far
is find insert position, and b; move elements over. As an example, suppose
the array was, I don't know, two, seven, 15, 24 and the element
to be inserted, let's say, is set ten. The rest of them don't matter. Here this is position number one, two, three, four, five. This is the array a,
and j equals five. All right? I would
like to do this, right? How does it work? The way we said is, let's scan the array
from the beginning, find out where this
element j needs to be inserted and once we find out where it
needs to be inserted, we can do a swap. This is one way of doing
things. All right? There is a more elegant
way of doing things, which is instead of looking for the position from the left, let us look for the position from the right and keep
swapping as we move along. How does this work?
This works like this. Let me start instead
of starting from 10, let me start already
from 24. All right? Now 24 and 10 are
in the wrong order. Let me swap them. I'm going to use this
operation called swap. A of i, A of j so
this is going to be a primitive operation and in
an array you have A of i, position number i, A of j, position number j and swap, let us say exchanges
these two elements. If I swapped A of 4 and
A of 5 in this array, then it's going to
swap 10 and 24. All right, so here's
what I'm going to do. I'm going to keep
scanning from the right. I'm going to keep
scanning from the right, and as long as this 10 is in the wrong position with
the element immediately, in my view, it's to the left, but when we flip the screen
it will be to your right. As long as this 10 is in
the wrong position to the element immediately
before it, we will swap. How does the pseudocode
for that look? The pseudocode looks like this. I'm going to try and write it in the pseudocode
convention of the book. Remember if you're
translating this, the Python code in Python arrays start from 0 to n minus
one, so you remember that. I'm going to write
down a for loop. What is this for
loop going to be? For i going from j
minus 1 down to 1. This is an example of
a for loop that says, let i run from j minus one all the way to one
and unlike Python, whenever I write for
loops in pseudocode, both ends are inclusive. Which means that it's going
to start from j minus 1, go all the way to one, do one and end. It's not going to end at
two like it does in Python, it's going to
actually end at one. The right hand side on the
left-hand side are both going to be included in
the range, unlike Python. This is the pseudocode
convention and it's meant to be free
of language barriers, so you should
translate it to Java or Haskell or Python or Scala, whatever you want, but we're going to write
it in this convention. This is the convention
of the book, so I'll stick to it. What is i? i is this pointer and J
is going to be at 10. All right, so i is going
to start from j minus 1. If j is five, then it's going to
start from 4,3,2, and 1.Now what am I going to do? Here, so far, what do I have? In the first instance, j equals 5, and i equals
4 in this example. I have 10 and 24 and 10. Let's look at A of i and A
of i plus 1.If A of i is greater than A of i plus 1, if they are in the wrong order, then swap A of i, A of i plus 1. Just swap them, just
exchange places. Else, if A of i is in the right place with
respect to A of i plus 1, then return or break. Break exits the
loop and we return. All right, what is this doing? Instead of iterating from the left like I described before, which makes sense for us, we are doing it from the right, and we are changing
positions as we go along.This is like a
dual purpose loop. Not only, does it find
the right position for j, for the element I want to insert. I claim that it also, magically inserts it and
moves the other elements out of the way until it
finds the right place. Let us see how it
works on this example. On this example,
we have the array, we are trying to insert
the element at position 5. All right, let's write down the array and
see what's happening. J is always going to be fixed, and it's going to be 5. Now, i is going to run from
4,3,2 and 1 very good. Now, when i equals
4, what is A of i? A of i is 24 and A
of i plus 1 is 10, so A of i is 24. A of i plus 1 is 10. Am I right? Now, 24 is greater than 10,so what it ends up doing is
it swaps the two places. It swaps 24 and 10, so the new array is going to look like 2,7,15,10, and 24. I'm going to put a box around the 10 so that you know
that it's actually in the wrong position and
now i becomes three. i becomes three,
so what is A of i? Is A of three, A of three is 15. A of i plus 1 is A
of four, look here, so 1,2,3,4, it's 10, and so what's going to happen? A of i is greater than A
of i plus one, so swap. When I swap, now I get
2,7,10,15,24, blah blah blah. All right, let me put
a box around the 10. Now i goes to two. When i goes to two,
what is A of i? A of i is A of 2 which is 7. A of i plus 1 is 10. Now what happens? Now, the if condition
is no longer true, so we fall into the
else condition. What are we saying in
the else condition? In the else condition
we are saying, that we found the right place. Now we just break, we just return out of the loop. In fact we are done.
In this example, by moving from right to left, I was able to both move
elements out of the way, move things to the right, move the element I
want to insert to the right position and remember when I say
right and left, I'm actually mirrored
over when you see me, so it's going to be
the opposite for you. But at least you get the idea
of what I'm talking about. You're moving elements to
the right position and you're moving the
other elements out of the way all in the same loop. If you find that the element is already
in the right position, you don't have to do
the rest of the loop, you can simply break. This is the insert procedure. Now that we have the
insert procedure done, we can do the rest
of insertion sort. What is insertion sort? Insertion sort is simple. For i going from one up to n, n being the length of the array. Here one and n are
both inclusive. Insert A, i, and that's it. In fact, to avoid confusion, let me call this j
and j. That's it. Move from the beginning
of the array to the end and insert. This is the whole of
insertion sort of an array A. Exactly as I described
in the previous lecture, now we have written
the pseudo-code. Remember there are some
pseudo-code conventions that are going to
be different from python so if you
translate the code verbatim, it's going to crash. One difference is we are going to assume that
array start at one, end at n. We know that
loops, for example, we are going to write these
funny loops up to and down to loops where both the ends
of the loop are inclusive. Whenever there are
differences between python, I will try and point
them out as we go along until you know these differences and you're careful implementing
them in Python. That is insertion
sort in a nutshell. The next lecture,
we're going to look at the important
question of how long does insertion sort take in the best case,
in the worst-case. Let's talk about that in the very next lecture.
See you then. In this lecture, we're
going to look at the complexity or
what is known as the computational complexity
of insertion sort. Now, what is
computational complexity? Computational complexity is a
fancy term for just saying, how much does it cost? In computation, we look at
cost in terms of resources consumed and the two
important resources are time; how much time does it
take to run and space. How much memory does it occupy? Because these are going to be bottlenecks for running
this code on large inputs. Now there is already
a problem here. When you are going
to measure time, you are really interested in measuring time on a
particular machine, but we are not designing this algorithm to
run on your laptop, my laptop, on a server, we're not specifying
how the algorithm runs. Time is going to be a function of which processor
I choose to run. Therefore, the time that I
measure today is going to be way faster than the run time
I measure six months ago. Not because the
algorithm changed, not because I improved things, but just because
the processor got faster because of
some other advances. I need time to be
measured in a way that's independent of the
processor, the clock speed. I don't want to say on a 1.8
megahertz processor running the Mac OS operating system with this much cache
and this much ram, it took this much time to run. That's too much details
and too complicated. I want a way to
measure time that's independent of the
processor that's being run. I also want to measure space complexity,
which is the memory. But then the memory usage depends on how big
these numbers are, where it's a 32-bit computer or a 64-bit computer so I
want to simplify it. I also want to make
it independent. This is number 1,
challenge number 1. Challenge number 2 is this algorithm is
not going to take the same time on all the arrays. There are certain arrays in which it's going to run faster, and certain arrays in which
it's going to run slower. There are some best cases
and some worst cases. I need to be confident
that I know what the turning time
is going to be in the best case and
in the worst case. Therefore we call
the best case as the best case complexity and the worst case is the
worst case complexity. Unusually we are interested in worst-case complexity because the best-case complexity
is too self-serving, it's often considered
too self-serving. Then there's a third
one called the average-case complexity, which we will revisit when we have a better grounding
in probability. We're not going to
visit average-case. How do we make the whole
thing independent of time? To do that, we just need to give some abstract cost to
certain operations. I can say comparison
takes one time unit. I don't know how much
that time unit is. But let me say comparison
takes one time unit. Let's say swap takes
one time unit. If you want to be more precise, you can call it C_1 time unit. Let's say a swap
takes C_2 time units. Let's say running a for loop, each iteration of the for loop, what does each iteration
of the for loop do? It decrements i by one, starts i with j, minus
one ends with j. Let's say each iteration
cost C_3 time units. Having said all of that, what is the cost of an
insert? It depends. There is the
best-case for insert, and there's the
worst-case for insert. What's the best-case for insert? I'm just studying insert and then I will study
insertion sort. What's the best-case for insert? The best-case for insert happens when you enter this loop. The moment you enter this loop, you check this condition, it's immediately
falls and you break. Let's say break costs C_3 time units,
something like that. This is going to be the
best-case for insert. When that happen? That happens in this
very specific situation. Whatever you are
trying to insert, let's say now you
have some elements, A1, A 2, A j minus one, A sub j. This is already sorted
in insertion sort, we assume it's already sorted. Now, when you are
trying to insert A sub j, this situation happens, Ai greater than A minus
A i plus one, is false, which means in this case, A sub i less than equal
to A sub i plus one. What happens can be
that A j is larger than A j minus one. I am going to write a
less than equal to, in which case A j minus one
is less than or equal to Aj. If the jth element is already greater than equal
to j minus one, which is when you get to break. What it means is that A j
doesn't need to be inserted, it's already in the
right inserted place. If it's already in the
right inserted place, then no work needs to be done, or some nominal small amount
of work needs to be done. The four loop immediately exits. How much is the best-case then? The best-case is going to
be something like C_3, the cost of doing
one iteration of the for loop for bookkeeping. C_1, the cost of
doing this compare. I think I should
have called this C_4 because C_3 is
already being used. Let me call this C_4. No problem. This is very, very simple and it later what we are
going to do is we're going to call this Theta
of one or big O of one. When we get to the big
O, big Theta notation, which you must have
already studied in your basic discrete math class. You would just write it as
big Theta one or big O one. It's a constant that is the extent to which
we care about it. That's how we achieve
independence by using the big O, big Theta notation,
and not worrying about constant factors.
That's the best-case. As I said, who cares
about the best-case? It's a little bit
too self-serving. What's the worst-case of insert? Well, the worst-case is when this iteration has to go all
the way from j minus one, j minus two, j minus three, all the way to one. When does that happen? That happens when
Aj's insert position is at the very beginning
of this array. If Aj's insert position is at the very beginning
of this array, that's when the
worst-case happens. What's the worst-case complexity? Well, in the worst case, this entire loop runs j minus 1 times j
minus 1 down to one. It runs j minus 1 times and each time it runs the compare, it runs the swap and it runs
the for loop iteration, whatever bookkeeping is
needed for a for loop. Here the answer is
and by the way, there needs to be okay. So the answer here is going
to be j minus 1 times c_1, which is the cost of doing
the compare plus c_2, the cost of doing the swap, plus c_3, the cost of
doing the for loop. The worst-case is j
minus 1 times this and when we are back next
week to doing the big O, big Theta notation, we're
simply going to write this as big Theta
of j or big O of j. Big O of j is the upper bond big Theta of j is
the exact answer. In this case it's
actually big Theta, that's a better thing to
say, you can also say big O. We'll see this next week, don't worry about it right now but I think you've
already seen it, so I'm just giving
you the flavor of it. This is the computational
complexity for insert. In the best case, it immediately
exists with a very, very small cost of
just three operations, in the worst case
it runs j minus 1 times and the complexity is something like j
minus 1 times c_2, c_1 plus c_2 plus c_3. What are we going to do next? Next, let's look
at insertion sort. What is insertion sort doing? It's running insert n
times from j equals 1 to n. What is the best case complexity
for insertion sort? In the best case complexity, each insert operation immediately
exits in one iteration. Each insert tickles
the best-case. If each insert tickles
the best-case, then you get the
best-case overall for insertion sort and
the best case would be n times the complexity of the best case for each insert, which is c_3 plus c_1 plus c_4. What kind of an array happens to solve
for this best case? The answer here is simple, you will notice
that the best case only happens if the array is already ascending sorted. If your array is already
sorted in ascending order, you will find that the
best case of insert happens in every call so the entire sorting
is done within n, n is the number of times
this outer j loop runs, times the cost of
each insert which is c_1 plus c_3, plus c_4. But again, best case is too self-serving so we really don't worry about the best-case. Too self-serving. If I was about to sell
insertion sort to you and I only talked
about the best case, you might sue me and say, ''Hey, in the worst case, you are so much worse than you advertise.'' Best-case usually we don't
do that analysis but though, to contrast it with
the worst case, I really like to have the
best case analysis as well. What's the worst case? Well, the worst case
is I run this insert, but each time I
run it for each j, I tickle the worst case
complexity, this guy. This is going to be
summation j going from 1-n and this is the worst case. Summation j going
from 1-n of what? Summation j going from 1-n, the cost of doing insert
with j in the worst-case, which is j minus 1, c_1 plus c_2, c_3. This is nothing but c_1 plus
c_2 plus c_3 taken out in common of 0 plus 1 plus 2, all the way to n minus 1 because j minus 1
summing up from 1-n. So this is n times n minus 1 over 2 times c_1
plus c_2 plus c_3. Then, once we are ready with our favorite big O notation
or big Theta notation, this is simply big
Theta n squared, or big O n squared and we're going to talk about
this in the very next week. We're going to get to
the big O notation and remember that's
how we are going to achieve this independence from the machine that you're
running this algorithm in. In the worst case, the algorithm runs something like n squared. N into n minus 1 by 2 times
c_1 plus c_2 plus c_3. In the best case, it's n
times c_3 plus c_1 plus c_4. Also, what is the array that causes always the worst
case of insert to happen? Well, if you think about it, you would notice that an array
that is descending sorted, that is sorted in exactly
the reverse order that you need it to be so
if you want it to be sorted in ascending order, suppose I give you
an array that's sorted in the descending order, that's always going to give you the worst case of insertion sort. Reason being that each element, A,j when you insert
needs to go all the way to the beginning because the
array is descending sorted. Each element A,j will
need to go all the way to the beginning so
it needs to move. That's a problem because
each time I call insert, I'm going to end up
running this loop entirely from j down to one, which gives me a
complexity that's n into n minus 1
over 2 times this if you want to be really precise and what we will do
is such precision achieves nothing
for us so we will simply call it n squared
and be done with it.