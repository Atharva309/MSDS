In this lecture, we
are going to talk about a sorting algorithm called Merge Sort as another example
of an algorithm that we will show how to analyze the correctness and the
running time briefly. I have provided
some notes where I have more details
on the pseudocode, the actual Python code, and the analysis
of this algorithm. Merge Sort is a
well-studied algorithm because it illustrates a paradigm that we will study later in this class called
divide and conquer. But before we go there, let's reiterate what is sorting? You're given a list, let's say of numbers. It could be a list of strings, but let's say it's a list
of numbers to keep life simple which is in any arbitrary order and your goal is to produce a list in
the ascending order, and that's called
the sorted list. We will assume that it's
sorting in ascending order. You can also do it in the descending order and the
algorithm doesn't change. So the basic idea of the Merge Sort algorithm is actually very nice and simple. Here's how it goes. What Merge Sort first does
is it takes an array, let's say it's an array here of size six or a list of size six, and it divides it into two parts. In this case, six
is an even number, so the two parts
are equal parts of three and three respectively
and when you divide, there is nothing to be done, it just looks at it as
two logical sub-arrays. Typically you don't create
these two sub-arrays afresh. You simply consider some left and right
pointers to signify the beginning and the end of the current region of interest as we will
show in a second. But let us say for now that
we divide it into two parts. Now you further subdivide each of these parts recursively
and now you have 1, 3 and minus 2, because three cannot be
divided perfectly into two. So you'll now have one part with two and another part at one. Now that your sub-arrays
are so small, of size two and size one, you know how to sort a list
with just two elements in it. If it has just two
elements in it, compare the first
element against the second and if they are not in
the right order, then swap. As an example, compare
one against three, it's in the right
order, so keep it. If your list is of size one, then that's trivial,
just keep it. Now here's an example
of a sub-list: 4, minus 1, which is
in the wrong order, so we swap the minus 1 and
the 4 and we create in place, the unsortedness, and now
you have corrected it. Last but not least 2
is just a singleton, a single list with a single
element, nothing to be done. Just leave it as it is. Now that we've reached this part then we call these
the base cases. So when you get to
lists that are so small that you know how to sort them through a few lines of code, you go ahead and do that. Now that you're done with these, you go ahead and merge. How do you merge?
So now you have 1, 3, which is a sub-list
and minus two. So the merged part of
the Merge Sort will take these two sub lists
and merge them back but doing so
in a sorted order. So minus 2, 1, 3, and now you have minus 1, 4 and 2, and so you
do minus 1, 2, 4. So you merge back
in a sorted order. Now, you might ask, since I already separated
at the very first step, why didn't I simply merge the two sub-lists at
the very first step? The reason is this, when I merge, I am going to assume that the two sub-arrays that I am
merging are already sorted. That's going to make
the merge algorithm particularly easy, that I am going to assume that the left and right
parts are already sorted and therefore I'm
merging two sorted arrays. So I'm merging not
arbitrary arrays but I'm merging sorted arrays. You will see that in a second that that's actually much easier. If your arrays are
already sorted, much easier to merge it
into a single sorted array. Likewise, Merge
Sort will take this and this half and
merge it into minus 2, minus 1, 1, 2, 3, and 4, which is your final sorted answer because there's no
more merge to be done. From a 1,000 feet, this is how merge sort
is going to look. Now, obviously, you can write the pseudocode and we'd use recursion to write
this pseudocode. When we start Merge Sort, we define an array, so we have an array
which is the array to be sorted and instead of
splitting the array, which is often going
to be expensive, we are simply going to define our current region that
we are going to work at as being delineated
by two pointers, being delineated by left, the left-hand pointer,
and the right. So my region of
interest is simply the sub-array between left and right, both ends inclusive. Initially when I
initialize this algorithm, left is initialized to zero and right is initialized
to n minus 1, where n is the length or the size of this array
that I'm interested in. So I can initialize
left and right to cover the entire array and then I
simply go ahead and recurse. Now, interestingly,
the simple cases are, if left is greater than
or equal to right, which means you
are ending up with a list which is singleton, which is left and
right are the same. Or something went wrong that your left is actually
greater than right, which signifies the
empty sub-array, then nothing to do, just return. On the other hand, if
left plus one is right, which means that you are in this situation where
left and right are just adjacent to each other and just one apart from each other, then this is simply
telling me that my sub-array has two elements. I look at whether they
are in the wrong order. If they are in the wrong order, I swap them and I can implement a simple swap
function that's just going to flip or swap the two elements left
and right, and I return. Now this has fixed it, I've locally fixed it, and this is another base case. This is a base case for empty
lists or lists of size 1, or arrays of size 1 and this is a base case for arrays of size 2. Now comes the interesting
part, which is the recursion. What I'm doing is I have my search region which goes
between left and right. I'm going to define the
middle of the search region, mid, which is the midpoint here. I'm going to recursively
call Merge Sort on the left half and I'm going to call Merge Sort on
the right half. I'm going to call Merge Sort
on left and mid and I'm going to call Merge Sort
on mid plus one and right. Now this left half is going to recurse all the way to the
base case and hopefully, there is a merge algorithm that combines it and
then it comes back, it's going to get sorted, the left half is
going to sort from left to mid, both ends inclusive. The right half going to
go from mid plus one to right and when you call the
second call to Merge Sort, that's going to sort
the right half. Now you call the merge algorithm and you simply
specify that, "Hey, merge algorithm,
here is the array from left to right that
I'm interested in. I split it at mid and I have sorted this half of the array from left to mid, and I have sorted the second half of the array from mid
plus one to right. Could you please merge them into a single array in place
and return it back to me?" That's the overall
Merge Sort algorithm and if you go back to the
previous slide, hopefully, you can see how this picture that I painted for you is being implemented here through this recursive pseudocode
that I have given you. I hope that's clear
how that's happening. If not, there is
a notebook or you can pause here and stare at it and try to implement it or look at the notebook
we have provided. If that is okay, let's move to the magic which is happening in the
merge procedure. This is going to be
the merge procedure. What's the setup for
this merge procedure? I have this entire array, but my focus is on the
sub-array right now, from left onto right. It's going to be on the
sub-array from left onto right and that's the region of interest that I'm going to focus on, everything else is
out of my focus. Left and right are
both inclusive. It includes left and
it includes right, so that's the region
I'm interested in. I know from context from
the previous slide, I already told you that this is split into two parts by mid. From left to mid plus one
is one sorted sub-array. It is sorted from left to mid. From left to mid both
points inclusive is sorted. Likewise, from mid plus one
to right is also sorted. But unfortunately, from left to right
itself is not sorted. As an example, here is, let me write it as two
distinct sub-arrays, left, let's say is minus 1, 2, 5, 15, 18, and this is mid. Then you have mid
plus one, which goes, let's say, let's do a better
example that's less trivial. Let's say, 3, 4, 7, 12, 16, 21. This is mid plus one to right. Now this entire thing is
part of the single array. What I would like to do is merge them so that when I return, I have the merged result of this. How would I do that? Now, the way I would
do it is like this. First of all, it's
very tricky to do merge in-place without creating
an extra temporary store. You can do it, but you have to implement Merge Sort in a different
way to do in-place merge, which is very tricky. Instead of that tricky
and hard algorithm, let me show you easier
algorithm where I create a temporary store, which is initially empty, but I'm going to keep the temporary result of the merge in here in
the temporary store. Then I will copy the temporary store back
into the array from left to right and it will
look as if I have fixed the array from left
to right by merging it. But I will have to create
this temporary store. It is an annoyance and it's one of these reasons
why Merge Sort is not as frequently
used as something else we will talk about
called QuickSort later on. Now the idea is very
simple of Merge Sort. Let's start by
implementing two indices, i and j, which are going to represent the current points
at which I am merging. I'm going to keep
iterating this loop, so this index i is going to move right and the index j
is going to move right, but they will move right
at different pace and so while i is still less
than or equal to mid, so while I haven't
exhausted the first half, and while j is less than or equal to right which means I haven't exhausted
the second half, let me do the following. Let me compare the
elements at i and j. Whichever one is smaller, let me copy it over to temp, and then that pointer, since i got copied over, now the new i moves to i plus 1, but since j did
not get copy over, j remains where it is. So this algorithm is just proceeding like
this and those are the two parts of the algorithm in the main while loop
of the algorithm, and so now i is here
pointing to the element two and two gets copied over because two
is less than three. Now i goes here and
points to element five, but three is less than five, so three gets copied over and j moves right
to the element four. Now four is again less than five, so four gets copied over, and j moves once more to the right and now
points to the element seven. Now five gets copied over because five
is less than seven. Now i moves to 15. Likewise, you can see
12 gets copied over, then 15 gets copied over, and then here 16
gets copied over, then j moves to 21, and 18 gets copied over. Now i has moved away, so you have completely written out the first
half of the array, but you still have parts
left in the second half. After the main
while loop is done, what merge does is it first checks if there are things left in the first half of the array to copy over
in which case I'm just going to iterate
through whatever's left and copy it over. If there are things left in the second half which means the j pointer is
less than the right, then I will copy that over and then I have
finished the merge. In this case, I will copy over 21 because this case
applies, the second case. Now I will copy back from
this temp_store back into my original array so
that I can fix everything, and when merge returns, the array from left
to right will be sorted as a single
sub-array or sub-list. That's kind of interesting. Okay, so hopefully this idea of merge is clear and natural. Let's probably do
one more example. Let's say this is an example, minus 1, 2, 3, 7, 14, 18, 21, and then
let's do the other one. Let's say 2, 7, 9, 10, 11, 12, 13, 20. So let's say this was left, this is mid, and this is right. Initially the i pointer is here and the j pointer is here, and the temporary store
is initially empty, so what we do is we
copy whichever one between the i and j pointer is pointing to the
smaller element, that element gets copied, and then i moves one position
to the left, so right here. Now both are the same, so it doesn't matter
which one you copy over, and then you get a
chance to copy over the j pointer and
that moves here. All right, now i pointer is
at three, j is at seven, so you copy over three, and then you move
the i pointer here, you copy over seven, and then you move
the i pointer here, and so j is still stuck here, so you copy over seven,
and that's done. Move the j pointer,
copy over nine, and then move the j
pointer, copy over 10. Remember, i pointer is at 14, so you copy over 10. You copy over 11, you copy over 12, you copy over 13, so
the j pointer keeps moving because the i pointer
is now pointing to 14. J pointer moves to 20, so i pointer now moves to 18, and then you copy over 20. Now at this point, the j pointer has moved past, so you stop and you see okay, there is a little bit
left here to copy, so you copy that, and then there is nothing left here to
copy, so you are done. So this is my temporary store and then you copy back
the temporary store into the original array and you know that this is going
to be the sorted version. Hopefully this example made it clear even though I went
through it rather fast, so then the question is what is the correctness argument
for merge procedure? Why is merged correct? Here's the idea. Let
us think about merge. There is the first
sub-array from left to mid inclusive and the second
sub-array from mid plus one all the way to right. Now, at any point in merge, let us say here is
where my i pointer is and here is where
my j pointer is. The first property
I will establish, is that the temporary
store, so far, has successfully merged the
sub-list from left to i minus 1 and the sub-list from right
mid plus 1 to j minus 1. So this sub-list A
and B, the merge, the actual merge of this little A sub-array
and B sub array, is in the temporary
store already. Now that's important. What you can show then is if
you iterate one more step, this process is still whole. Suppose the array at i is
less than the array of j. If the array of i is less
than the array of j, then it's clear that the
ith element here should go into merge before
the jth element goes. What the algorithm does then maintains that the A region
now expands to grow up to i, and i becomes i plus 1
and so this now becomes the new merge of this new region which grows and B
remains the same. You can see that the algorithm
continues to maintain this property throughout
the iteration. When the merge is done, when is the overall
while loop done? When you have exhaustively copied one of the
regions into the array, while the other region still
has stuff left to copy. Well then, what you
can show is that this entire A and this part of B have been
merged successfully. What you can show
is all that remains over is to copy
what remains of A. Now you can complete this correctness proof by noting one more
interesting point, which is everything
that you have in temporary store is already
the merge of A and B. But everything here is less than or equal to whatever
is remaining to be copied. You can use these facts to
finish up the argument, but you can see how the
correctness argument depends on maintaining a key
invariant property. We will use this pattern in many algorithms and we
will show in our notebook, I have spelled this out in further detail and
explained all of this. I think it's rather tedious for me to go over that in detail, so I invite you to examine the Jupiter notebook
that has been posted along with this lecture. How fast does merge run in? Suppose I had input
array from left to mid, and from mid to right, then you can see that suppose
I have my temporary store, you can show that
every element gets copied into the temporary
store exactly once, so every element gets
considered and gets copied into temporary
store exactly once. What you can show is
that for every copy, for every element that I
copy into temporary store, I do a comparison between one element of left and one
element in the right half. This means that there are two comparison operators involved in filling every element. For filling every element, there are about two comparisons
plus one copy operator, and so let's say that's
roughly three time units, but it's a constant. Each element in temporary store, requires three time units and the total number
of elements in temporary store is
just right minus left. The running time
of merge is simply the sum of the sizes
of the two halfs, which means that it's
simply right minus left, typically you would say
right minus left plus 1, because right and left
are inclusive so that our right minus left
plus 1 elements. Running time of merge is right minus left plus 1. This is of important. Merge runs in time
that's proportional and this is a big
Theta or a big O , however you want to. If you are just looking
at upper bounds, big Theta would
require you to also show examples like a lower bound where you show
examples of inputs to merge that can take
this much time, and that's not difficult to show, that's actually
quite easy to show. The running time
of merges is this. What you get from here
is very interesting. You can now analyze the
running time of Merge Sort. Let me do it at a high level. Here is the high level
picture of Merge Sort. Suppose you started Merge Sort, with an initial array of size n, and for the sake of simplicity, let's say n is a perfect power of two so that every time I divide, I end up with an even number. So what happens then
is n gets divided into two sub-arrays
of n by 2 and n by 2. Okay? Now that gets divided further into
sub-arrays of n by 4, n by 4, and there
are four of them. There are 4 sub-arrays of n by 4 we will encounter
using merge sort. Four times n by 4. You keep going until you get
to sub-arrays of size 1. Because this is perfectly
even at every stage, you will end up with
sub-arrays of size 1. Here you will end up with
n sub-arrays of size. Each of these costs nothing, or costs one step, which is just a recursion step. Let's not give a cost to
the splitting just yet. Let's say we can
just split, okay? Now the interesting
thing is the merged, this merge here between the first and the second
sub-array of size one, let's say costs 2 units. You'll have 2 unit costs. So now you have time-span. The cost of this merge is 2 units and you're
doing it for n by 2, of these problems of size 2. At this step, if you look
at this step in one shot, the cost is 2 times n by 2. Now you can say you have, you can merge 2
subarrays of size 2. So the cost here
would be 4 units, but you are doing
n by 4 of those. Then eventually you will have
2 subarrays of size n by 2. The cost at this step
is 2 times n by 2, okay, and eventually end up
at 1, which is the answer. If you want to analyze the cost, then, at each step, you can say the total cost
of all the merges I do at this level is n.
The total cost of merges I do at this level is n. The total cost of merges
I do at this level is n and n. What you can claim here, and this is, you
have to see this. Is the cost of merge sort. Is n times the number of levels, the number of recursion
levels I need to undergo, and what's the number of levels? The number of levels
is the number of steps it takes when I go n to n by 2 to n by 4, all the way to 1. How many steps does it take? That's the number of levels. The cost of merge sort is n
times the number of levels. How many levels are there? If n is 2^k, then in the first level
you get 2^k minus 1. In the second level
you get 2^k minus 2, all the way to 2^k minus k. You can conclude that the number
of levels is actually k, that n is 2^k, it's n times k. But what is k? k is just log n. So
you can conclude that the time taken by merge-sort is proportional to n log n
plus some constant factors, which I don't get into. If you did a careful analysis, you can show that
it's n log n. Okay, the base doesn't
matter if you put a theta or if you say
it's proportional. The analysis that I've showed here is just showing
an upper bound, Big O of n log n.
This is interesting. How does n log n
compare with n squared, which is the complexity
of insertion sort. Let's just compare these numbers, n log n and n squared. Let's say n equals 1 billion. That's about 2^30. All right, so how
long does it take to do an order n
squared algorithm? About 2^60 or 10^18, it's roughly a
billion iterations. As n log n is more like
a more gentle number, it's like 30 times 2^30. This is like 30 billion. This is nice. You can still consider running 30 billion
operations on a computer. This is completely out of the way times you would
need on existing computers. A single-core existing
computer will not be able to tackle
an algorithm like this. By the way, Insertion Sort, which we saw has this complexity, whereas Merge Sort
has this complexity. This gives you an idea
of how the complexity of a sorting algorithm
is so important because you can actually do
it very, very faster, right? About a billion times
faster, billion x faster. Merge Sort would
become a billion times faster than an Insertion Sort. Of course, there's
a constant factor which we are not talking about. So it won't quite be a billion, but it's in that
order of magnitude. Which is important to understand. I hope this was clear
and informative and I will post the notebook so that you can actually
look at the details of this, and this would be important in the very first week to understand these
details. Thank you.