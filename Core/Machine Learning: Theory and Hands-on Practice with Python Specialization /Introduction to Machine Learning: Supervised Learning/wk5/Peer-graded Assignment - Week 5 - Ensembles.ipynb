{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475bf108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "438fbad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image\n",
    "# Image(filename=\"images/2a.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93b20e9",
   "metadata": {},
   "source": [
    "Upload a screenshot of the fit function from the class adaboost. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91018985",
   "metadata": {},
   "source": [
    "```\n",
    "def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Train AdaBoost classifier on data. Sets alphas and learners. \n",
    "        \n",
    "        Args:\n",
    "            X_train (ndarray): [n_samples x n_features] ndarray of training data   \n",
    "            y_train (ndarray): [n_samples] ndarray of data \n",
    "        \"\"\"\n",
    "\n",
    "        # =================================================================\n",
    "        # TODO \n",
    "\n",
    "        # Note: You can create and train a new instantiation \n",
    "        # of your sklearn decision tree as follows \n",
    "        # you don't have to use sklearn's fit function, \n",
    "        # but it is probably the easiest way \n",
    "\n",
    "        # w = np.ones(len(y_train))\n",
    "        # h = clone(self.base)\n",
    "        # h.fit(X_train, y_train, sample_weight=w)\n",
    "        \n",
    "        # =================================================================\n",
    "        \n",
    "        w = np.ones(len(y_train))\n",
    "        w /= np.sum(w)\n",
    "        for k in range(self.n_learners):\n",
    "            h = clone(self.base)\n",
    "            h.fit(X_train, y_train, sample_weight=w)\n",
    "            y_pred = h.predict(X_train)\n",
    "            err = self.error_rate(y_train, y_pred, w)\n",
    "            self.alpha[k] = 0.5 * np.log((1 - err) / err)\n",
    "            w *= np.exp(- self.alpha[k] * y_train * y_pred)\n",
    "            w /= np.sum(w)\n",
    "            self.learners.append(h)\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69dbc03",
   "metadata": {},
   "source": [
    "Upload a screenshot of the error_rate function from the class adaboost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55be589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ea9daeb",
   "metadata": {},
   "source": [
    "Upload a screenshot of using the fit function to fit the Adaboost classifier with 300 base decision stumps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2954da9b",
   "metadata": {},
   "source": [
    "```\n",
    "clf = AdaBoost(n_learners=300, base=DecisionTreeClassifier(max_depth=1))# TODO \n",
    "\n",
    "clf.fit(data.x_train, data.y_train) \n",
    "\n",
    "print(clf.alpha)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c92358",
   "metadata": {},
   "source": [
    "Part C has the following directions:\n",
    "\n",
    "Once your predict function is written up, you need to test the scores on the function. To do this compute the scores on the prediction in the score function. Use the score function to then complete staged_score to collect the scores for every boosting iterations. Plot the misclassification error for train and test sets (misclassification error = 1- score).\n",
    "\n",
    "Upload a screenshot or copy of your code for the misclassification error for the train and test sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71fa829",
   "metadata": {},
   "source": [
    "```\n",
    "train_scores = clf.staged_score(data.x_train, data.y_train) # TODO \n",
    "valid_scores = clf.staged_score(data.x_test, data.y_test) # TODO \n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,4))\n",
    "ax.plot(1 - train_scores, label=\"train\")\n",
    "ax.plot(1 - valid_scores, label=\"valid\")\n",
    "ax.set_xlabel(\"boosting iteration\", fontsize=16)\n",
    "ax.set_ylabel(\"misclassification error\", fontsize=16)\n",
    "ax.grid(alpha=0.25)\n",
    "ax.legend(loc=\"upper right\", fontsize=20)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b0bbe0",
   "metadata": {},
   "source": [
    "Part C has the following directions:\n",
    "\n",
    "Once your predict function is written up, you need to test the scores on the function. To do this compute the scores on the prediction in the score function. Use the score function to then complete staged_score to collect the scores for every boosting iterations. Plot the misclassification error for train and test sets (misclassification error = 1- score).\n",
    "\n",
    "Upload a screenshot or copy of your plot of the misclassification error for the train and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7c4848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc64c2c1",
   "metadata": {},
   "source": [
    "Problem 2 Building a Random Forest Classifier to Classify MNIST Digits 3 and 8 has the following directions for Part A:\n",
    "\n",
    "Complete the create_tree function to build a new tree trained on a subset of data. Within this function a decision tree classifier is built and trained on the subset of data with the subset of features. \n",
    "\n",
    "Upload a screenshot of your create_tree function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256e9dae",
   "metadata": {},
   "source": [
    "```\n",
    "def create_tree(self,i):\n",
    "        \"\"\"\n",
    "        create a single decision tree classifier\n",
    "        \"\"\"\n",
    "        \n",
    "        idxs = np.random.permutation(len(self.y))[:self.sample_sz]\n",
    "        idxs = np.asarray(idxs)\n",
    "\n",
    "        f_idxs = np.random.permutation(self.x.shape[1])[:self.n_features]\n",
    "        f_idxs = np.asarray(f_idxs)\n",
    "        \n",
    "        \n",
    "        if i==0:\n",
    "            self.features_set = np.array(f_idxs, ndmin=2)\n",
    "        else:\n",
    "            self.features_set = np.append(self.features_set, np.array(f_idxs,ndmin=2),axis=0)\n",
    "        \n",
    "        # TODO: build a decision tree classifier and train it with x and y that is a subset of data (use idxs and f_idxs)\n",
    "        \n",
    "\n",
    "        \n",
    "        clf = DecisionTreeClassifier(max_depth = self.max_depth, min_samples_leaf=self.min_samples_leaf)\n",
    "                \n",
    "        x, y = self.x[idxs][:,f_idxs], self.y[idxs]\n",
    "        clf.fit(x,y)\n",
    "        \n",
    "        return clf\n",
    "    \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6bad7a",
   "metadata": {},
   "source": [
    "Problem 2 Building a Random Forest Classifier to Classify MNIST Digits 3 and 8 has the following directions for Part B:\n",
    "\n",
    "In this part you will have to complete three steps. The first step is:\n",
    "1. Complete the predict function in RandomForest class so as to make predictions using just the features. \n",
    "\n",
    "Upload a screenshot of your predict function here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1950263d",
   "metadata": {},
   "source": [
    "```\n",
    "def predict(self, x):\n",
    "        \n",
    "        # TODO: create a vector of predictions  and return\n",
    "        # You will have to return the predictions of the final ensembles based on the individual trees' predicitons\n",
    "        \n",
    "        \n",
    "        ### BEGIN SOLUTION\n",
    "        predict_list= [t.predict(x[:,self.features_set[i,:]]) for i, t in zip(range(self.features_set.shape[0]),self.trees) ]\n",
    "        predict_array=np.array(predict_list).transpose()\n",
    "        print(predict_array.shape)\n",
    "        predict_minus1 = np.array((predict_array==-1).sum(axis=1)/predict_array.shape[1])\n",
    "        predict_ = predict_minus1\n",
    "        predict_ = np.empty(predict_minus1.shape)\n",
    "        predict_[(predict_minus1 > 0.5)] = -1\n",
    "        predict_[(predict_minus1 <= 0.5)] = 1\n",
    "        \n",
    "        return predict_\n",
    "        ### END SOLUTION\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b671d18",
   "metadata": {},
   "source": [
    "Problem 2 Building a Random Forest Classifier to Classify MNIST Digits 3 and 8 has the following directions for Part B. The third step is:\n",
    "\n",
    "3. Build a random forest classifier and train it on the MNIST data to classify 3s and 8s in the cell below. Then see how the classifier performs on the test data by computing the misclassification error. (Remember: error = 1-score)\n",
    "\n",
    "Upload a copy of your code for this task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90d8280",
   "metadata": {},
   "source": [
    "```\n",
    "# TODO: build a random forest classifier and make predictions\n",
    "\n",
    "\n",
    "clf = RandomForest(data.x_train, data.y_train,int(np.round(data.x_train.shape[0]*0.7)) )\n",
    "\n",
    "y_pred=clf.predict(data.x_test)\n",
    "\n",
    "pred_score = 1- clf.score(data.x_test, data.y_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a539404d",
   "metadata": {},
   "source": [
    "Problem 2 Building a Random Forest Classifier to Classify MNIST Digits 3 and 8 has the following directions for Part B. The third step is:\n",
    "\n",
    "3. Build a random forest classifier and train it on the MNIST data to classify 3s and 8s in the cell below. Then see how the classifier performs on the test data by computing the misclassification error. (Remember: error = 1-score)\n",
    "\n",
    "What was the misclassification error for your random forest classifier? How did the misclassification error for the random forest classifier compare to the adaBoost classifier? \n",
    "\n",
    "Type your answer in the rich text field."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
